---
title: (VGG) Very Deep Convolutional Networks for Large-Scale Image Recognition
author: noru
date: 2025-09-01 12:00:00 +0900
categories: [AI-ML-DL, Paper Review]
tags: [VGG, CNN, Deep Learning, Image Recognition, Computer Vision]
pin: false
math: true
mermaid: false
image:
  path: /assets/img/posts/vcg/Pasted_image_20250901131049.png
  alt: VGG Architecture
---

### 🔗 출처
> https://arxiv.org/abs/1409.1556

---

## 🧩 방법론

**by …**

💡 참고: (부가 설명 작성)

---

# 📌 논문

## 💡 요약

**by Gemini**
#### 1. 깊이(Depth)의 중요성 강조

이 연구의 가장 핵심적인 기여는 컨볼루션 네트워크(ConvNet)의 깊이가 성능에 미치는 영향을 체계적으로 평가한 것입니다. 저자들은 다른 아키텍처 매개변수들을 고정한 채, 컨볼루션 레이어를 점진적으로 추가하여 네트워크의 깊이를 11개에서 19개의 가중치 레이어까지 늘렸습니다. 실험 결과, 네트워크의 깊이가 깊어질수록 분류 오류가 감소하는 경향을 보였으며, 이는 깊이가 이미지 인식 성능에 매우 중요한 요소임을 입증합니다.

#### 2. 3×3의 작은 컨볼루션 필터 사용

VGG Net의 주요 아키텍처 설계 원칙은 전체 네트워크에 걸쳐 매우 작은

3×3 컨볼루션 필터를 일관되게 사용한 것입니다. 이는 다음과 같은 장점을 가집니다:

- **효과적인 수용 영역(Receptive Field) 확장**: 3×3 필터 두 개를 쌓으면 5×5 필터 하나의 수용 영역과 동일하고, 세 개를 쌓으면 7×7 필터 하나의 수용 영역과 동일한 효과를 냅니다.
    
- **비선형성 증가**: 여러 개의 레이어를 쌓으면서 더 많은 ReLU 활성화 함수를 사용하게 되어 결정 함수(decision function)를 더 판별력 있게 만듭니다.
    
- **파라미터 수 감소**: 하나의 큰 필터를 사용하는 것보다 여러 개의 작은 필터를 사용하는 것이 파라미터 수를 줄여줍니다. 예를 들어, 3개의
    
    3×3 컨볼루션 레이어는 1개의 7×7 레이어보다 파라미터 수가 81% 더 적습니다. 이는 모델의 일반화 성능을 높이는 정규화(regularisation) 효과를 가져옵니다.
    

#### 3. 일관된 네트워크 구성

VGG Net은 깊이를 제외하고는 매우 일관되고 간단한 구조를 따릅니다.

- **입력**: 훈련 중에는 고정된 크기의 224×224 RGB 이미지를 입력으로 사용합니다. 유일한 전처리 과정은 훈련 세트의 평균 RGB 값을 각 픽셀에서 빼는 것입니다.
    
- **컨볼루션 및 풀링**: 3×3 컨볼루션 레이어 스택 뒤에 2×2 맥스 풀링(max-pooling) 레이어가 이어지는 구조가 반복됩니다. 맥스 풀링을 거칠 때마다 채널 수는 2배씩 증가하여 512개까지 늘어납니다.
    
- **완전 연결 레이어(Fully-Connected Layers)**: 컨볼루션 레이어 스택 다음에는 3개의 완전 연결 레이어가 위치합니다. 처음 두 레이어는 각각 4096개의 채널을 가지며, 마지막 레이어는 1000개의 클래스를 분류하기 위한 1000개의 채널을 가집니다.
    

#### 4. 훈련 및 평가 기법

- **훈련(Training)**: 훈련은 모멘텀(momentum)을 사용한 미니배치 경사 하강법(mini-batch gradient descent)으로 수행됩니다. 정규화를 위해 가중치 감소(weight decay, L2 페널티)와 처음 두 개의 완전 연결 레이어에 드롭아웃(dropout)이 적용되었습니다.
    
- **가중치 초기화**: 깊은 네트워크의 불안정한 기울기(gradient) 문제를 해결하기 위해, 상대적으로 얕은 네트워크(A)를 먼저 무작위 초기화로 훈련시킨 후, 더 깊은 아키텍처의 초기 레이어들을 이 얕은 네트워크의 가중치로 초기화했습니다.
    
- **스케일 지터링(Scale Jittering)**: 훈련 시 입력 이미지의 가장 작은 변의 길이를 특정 범위([Smin​,Smax​]) 내에서 무작위로 샘플링하여 리스케일링했습니다. 이는 모델이 다양한 크기의 객체를 인식하도록 훈련하는 데이터 증강(augmentation) 기법으로, 고정된 크기의 이미지를 사용하는 것보다 훨씬 좋은 결과를 가져왔습니다.
    
- **테스트(Testing)**: 테스트 시에는 완전 연결 레이어를 컨볼루션 레이어로 변환하여(fully-convolutional), 이미지 전체에 대해 조밀하게(densely) 네트워크를 적용합니다. 이를 통해 여러 크롭(crop)을 잘라내어 각각 평가하는 방식보다 효율적으로 평가를 수행할 수 있습니다. 또한, 여러 테스트 스케일(Q)에 대해 평가하고 그 결과를 평균 내어 성능을 향상시켰습니다.
💡 참고: (요약 보충 설명)

---

## 📚 정리

### 📌 제목
#### VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION  
Authors: Karen Simonyan, Andrew Zisserman
### 번역
대규모 이미지 인식을 위한 매우 깊은 합성곱 신경망(Very Deep Convolutional Networks for Large-Scale Image Recognition)

---

### 🌟 초록

### 번역
본 연구에서는 합성곱 신경망(Convolutional Network)의 깊이가 대규모 이미지 인식 정확도에 미치는 영향을 조사한다. 우리의 주요 기여는 매우 작은 (3×3) 합성곱 필터를 사용하는 아키텍처를 기반으로, 네트워크 깊이를 16–19개의 가중치 층으로 확장함으로써 기존 최첨단 구성보다 상당한 성능 향상을 이끌어낼 수 있음을 철저히 평가한 것이다. 이러한 발견은 우리가 ImageNet Challenge 2014에 제출한 모델의 기반이 되었으며, 그 결과 분류(classification)와 위치 추정(localisation) 트랙에서 각각 2위와 1위를 차지했다. 또한, 우리의 표현은 다른 데이터셋에서도 잘 일반화되어 최첨단 결과를 달성함을 보였다. 우리는 심층 시각 표현 연구를 촉진하기 위해 가장 성능이 좋은 두 가지 ConvNet 모델을 공게했다.
### 내용
* **CNN의 깊이**가 대규모 이미지 인식 정확도에 미치는 영향 조사 -> 16-19층으로 확장하여 성능 향상
- **작은 커널(3x3)**
### 주요 포인트
| 항목      | 내용                                               |
| --------- | -------------------------------------------------- |
| 데이터셋  | ImageNet (ILSVRC 2014)                             |
| 모델 구조 | 3×3 합성곱 필터 기반, 깊이 16~19층                 |
| 학습 방법 | 기존 ConvNet 훈련 절차 기반                        |
| 평가 지표 | Classification/Localization Error (Top-1, Top-5)   |
| 결과 해석 | 깊이 증가가 성능 향상으로 이어짐, 일반화 성능 우수 |

---

### 📌 서론 & 결론 & 고찰

### 번역
#### 서론
합성곱 신경망(ConvNets)은 최근 대규모 이미지 및 비디오 인식에서 큰 성공을 거두었는데(Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014; Simonyan & Zisserman, 2014), 이는 ImageNet(Deng et al., 2009)과 같은 대규모 공개 이미지 저장소와 GPU 또는 대규모 분산 클러스터(Dean et al., 2012)와 같은 고성능 컴퓨팅 시스템 덕분에 가능해졌다. 특히, ImageNet Large-Scale Visual Recognition Challenge (ILSVRC, Russakovsky et al., 2014)는 고차원 얕은 특징 인코딩(Perronnin et al., 2010, ILSVRC-2011 우승)부터 심층 ConvNets(Krizhevsky et al., 2012, ILSVRC-2012 우승)까지 여러 세대의 대규모 이미지 분류 시스템의 시험대로서 중요한 역할을 해왔다.

ConvNets가 컴퓨터 비전 분야에서 점차 보편화되면서, Krizhevsky et al. (2012)의 원래 아키텍처를 개선하여 더 나은 정확도를 얻으려는 시도가 이루어졌다. 예를 들어, ILSVRC-2013에서 가장 성능이 좋았던 제출작들은 첫 번째 합성곱 층의 수용영역 크기(receptive window size)를 더 작게 하고 stride를 줄였다(Zeiler & Fergus, 2013; Sermanet et al., 2014). 또 다른 개선 방향은 네트워크를 이미지 전체와 다중 스케일에서 조밀하게 학습 및 테스트하는 것이었다(Sermanet et al., 2014; Howard, 2014). 본 논문에서는 ConvNet 아키텍처 설계의 또 다른 중요한 측면인 <strong>깊이(depth)</strong>에 초점을 맞춘다. 이를 위해 아키텍처의 다른 매개변수들은 고정하고, 모든 층에서 매우 작은 (3×3) 합성곱 필터를 사용함으로써 점차 합성곱 층을 추가하여 네트워크 깊이를 늘린다.

그 결과, 우리는 ILSVRC 분류 및 위치 추정 작업에서 <strong>최첨단 정확도</strong>를 달성할 뿐만 아니라, 다른 이미지 인식 데이터셋에도 적용 가능한 상당히 더 정확한 ConvNet 아키텍처를 제안한다. 심지어 비교적 단순한 파이프라인(예: 심층 특징을 선형 SVM으로 분류, 미세조정 없이)에서도 뛰어난 성능을 발휘한다. 우리는 연구 촉진을 위해 가장 성능이 좋은 두 가지 모델을 공개했다.
#### 결론
본 연구에서는 대규모 이미지 분류를 위해 최대 19개의 가중치 층을 가진 매우 깊은 합성곱 신경망을 평가하였다. 그 결과, <strong>표현의 깊이</strong>가 분류 정확도 향상에 유리하며, 기존 ConvNet 아키텍처(LeCun et al., 1989; Krizhevsky et al., 2012)의 구조를 크게 확장하는 것만으로도 ImageNet 챌린지 데이터셋에서 <strong>최첨단 성능</strong>을 달성할 수 있음을 보였다. 또한 부록에서는, 제안한 모델이 얕은 시각 표현을 기반으로 한 더 복잡한 인식 파이프라인과 비교해도 동일하거나 더 나은 성능을 보이며, 다양한 작업과 데이터셋에 잘 일반화됨을 보였다. 우리의 결과는 시각 표현에서 <strong>깊이의 중요성</strong>을 다시 한 번 확인시켜준다.
### 내용
- AlexNet의 개선방향
	- 첫 번째 conv의 win.을 더 작게하고, stride를 줄임
	- 이미지 전체와 다중 스케일에서 조밀하게 학습 및 테스트
- 본 논문은 **깊이**에 초점을 맞춤, 따라서 다른 매개변수들은 고정하고, 모든 층에서 매우 작은 커널을 사용하고, layer을 추가하여 네트워크의 깊이를 늘린다.
- **표현의 깊이의 증가가 결론적으로 높은 정확도**를 가지게 됨
	- 분류 정확도 향상에 유리
- AlexNet의 깊이를 확장하는 것만으로도 성능이 높아짐
### 포인트
| 항목      | 내용                                                     |
| --------- | -------------------------------------------------------- |
| 데이터셋  | ImageNet (ILSVRC 2012–2014)                              |
| 모델 구조 | 3×3 필터 기반, 16–19층 ConvNet                           |
| 학습 방법 | 기존 ConvNet 학습 프로토콜 유지, 깊이만 확장             |
| 평가 지표 | Top-1, Top-5 error (classification), localisation error  |
| 결과 해석 | 깊이가 성능 향상 핵심, 복잡한 구조 없이도 성능 개선 가능 |

---

## 🔬 실험과정

### 📚 2. CONVNET CONFIGURATIONS

### 번역
ConvNet 깊이를 증가시킴으로써 발생하는 개선 효과를 공정하게 측정하기 위해, 우리의 ConvNet 층 구성은 모두 동일한 원칙을 따른다(Ciresan et al., 2011; Krizhevsky et al., 2012에서 영감을 받음). 본 장에서는 먼저 ConvNet 구성의 일반적인 레이아웃을 설명(2.1절)하고, 이어서 평가에 사용된 구체적인 구성들을 자세히 설명(2.2절)한다. 마지막으로 우리의 설계 선택을 기존 연구와 비교하여 논의한다(2.3절).
#### 2.1. Architecture
훈련 과정에서 ConvNet의 입력은 고정 크기 224 × 224 RGB 이미지이다. 우리가 수행하는 전처리는 훈련 세트에서 계산된 평균 RGB 값을 각 픽셀에서 빼는 것뿐이다. 이미지는 합성곱(convolutional, conv.) 층의 스택을 거치며, 여기서 우리는 매우 작은 수용영역(receptive field)을 가진 3 × 3 필터를 사용한다(이는 좌/우, 상/하, 중앙의 개념을 포착할 수 있는 최소 크기이다). 하나의 설정에서는 1 × 1 합성곱 필터도 사용하는데, 이는 입력 채널에 대한 선형 변환(비선형성이 뒤따름)으로 볼 수 있다. 합성곱 stride는 1 픽셀로 고정되며, 합성곱 층 입력의 공간 패딩은 합성곱 후에도 공간 해상도가 보존되도록 설정된다. 즉, 3×3 합성곱 층의 경우 패딩은 1 픽셀이다. 공간 풀링은 다섯 개의 최대 풀링(max-pooling) 층에서 수행되며, 이는 일부 합성곱 층 뒤에 배치된다(모든 합성곱 층 뒤에 배치되는 것은 아니다). 최대 풀링은 2×2 픽셀 윈도우에 대해 stride 2로 수행된다.

합성곱 층 스택(아키텍처마다 깊이가 다르다)은 세 개의 완전연결(Fully-Connected, FC) 층에 의해 이어진다. 첫 번째와 두 번째 FC 층은 각각 4096 채널을 가지고, 세 번째 FC 층은 1000-클래스 ILSVRC 분류를 수행하므로 1000 채널(각 클래스당 하나씩)을 가진다. 마지막 층은 soft-max 층이다. 완전연결 층의 구성은 모든 네트워크에서 동일하다.

모든 은닉층은 정류(Rectification) 비선형성(ReLU (Krizhevsky et al., 2012))을 사용한다. 주목할 점은 (하나의 예외를 제외하고) 우리의 네트워크들은 Local Response Normalisation (LRN) 정규화를 포함하지 않는다는 것이다(Krizhevsky et al., 2012). 이는 4장에서 보여주듯이 ILSVRC 데이터셋에서 성능 향상에 도움이 되지 않으며, 오히려 메모리 소비와 계산 시간을 증가시키기 때문이다. 필요한 경우 LRN 층의 파라미터는 (Krizhevsky et al., 2012)와 동일하다.
#### 2.2. Configurations
이 논문에서 평가된 ConvNet 구성은 표 1에 각 열(column)별로 요약되어 있다. 이후 네트워크들은 이름(A–E)으로 지칭된다. 모든 구성은 2.1절에서 설명한 일반적인 설계를 따르며, 차이점은 깊이에 있다: 네트워크 A는 11개의 가중치 층(합성곱 8개와 FC 3개), 네트워크 E는 19개의 가중치 층(합성곱 16개와 FC 3개)을 가진다. 합성곱 층의 너비(채널 수)는 비교적 작으며, 첫 번째 층에서 64로 시작해 각 최대 풀링 층 이후 두 배로 증가하여 최종적으로 512에 이른다
표 2에는 각 구성의 파라미터 수가 보고되어 있다. 깊이가 상당히 깊음에도 불구하고, 우리의 네트워크에서 파라미터 수는 더 얕은 네트워크가 더 큰 합성곱 층 너비와 수용영역을 사용할 때보다 많지 않다(예: (Sermanet et al., 2014)의 1억 4400만 개 파라미터).
#### 2.3. Discussion
우리의 ConvNet 구성은 ILSVRC-2012 (Krizhevsky et al., 2012)와 ILSVRC-2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014) 대회의 상위 제출작들과는 상당히 다르다. 그들은 첫 번째 합성곱 층에서 비교적 큰 수용영역을 사용했는데(예: Krizhevsky et al., 2012는 11×11 stride 4, Zeiler & Fergus, 2013 및 Sermanet et al., 2014는 7×7 stride 2), 우리는 전체 네트워크에서 매우 작은 3×3 수용영역을 사용하며, 입력의 모든 픽셀에서 stride 1로 합성곱을 수행한다. 두 개의 3×3 합성곱 층을 쌓으면(중간에 공간 풀링 없음) 효과적인 수용영역은 5×5가 되며, 세 개를 쌓으면 7×7 수용영역이 된다.

그렇다면 단일 7×7 층 대신 세 개의 3×3 층을 쌓음으로써 얻는 것은 무엇인가? 첫째, 비선형 정류층이 하나가 아닌 세 개가 포함되어 의사결정 함수가 더 판별력이 높아진다. 둘째, 파라미터 수가 줄어든다. 예를 들어 입력과 출력이 모두 C 채널일 때, 세 층의 3×3 합성곱 스택은 3(3^2C^2) = 27C^2개의 가중치를 가지지만, 단일 7×7 층은 7^2C^2 = 49C^2개를 필요로 한다. 즉, 81% 더 많다. 이는 7×7 합성곱 필터에 대해 3×3 필터를 통한 분해(중간에 비선형성 포함)를 강제하는 일종의 정규화로 볼 수 있다.

1×1 합성곱 층의 도입(구성 C, 표 1)은 합성곱 층의 수용영역에 영향을 주지 않고 의사결정 함수의 비선형성을 증가시키는 방법이다. 우리의 경우 입력과 출력 채널 수가 같으므로 1×1 합성곱은 본질적으로 동일 차원 공간으로의 선형 투영이지만, 정류 함수에 의해 비선형성이 추가된다. 1×1 합성곱 층은 최근 Lin et al. (2014)의 “Network in Network” 아키텍처에서도 사용되었다.

작은 크기의 합성곱 필터는 이전에 Ciresan et al. (2011)에 의해 사용되었지만, 그들의 네트워크는 우리 것보다 훨씬 얕았으며 대규모 ILSVRC 데이터셋에 대해 평가하지 않았다. Goodfellow et al. (2014)는 11층 딥 ConvNet을 거리 번호 인식(street number recognition) 과제에 적용하여 깊이가 증가하면 성능이 향상됨을 보였다. GoogLeNet (Szegedy et al., 2014)은 ILSVRC-2014 분류 과제의 상위 제출작으로, 독립적으로 개발되었지만 매우 깊은 ConvNet(22층)과 작은 합성곱 필터(3×3뿐 아니라 1×1, 5×5 사용)에 기반한다는 점에서 유사하다. 그러나 GoogLeNet의 네트워크 토폴로지는 우리 것보다 훨씬 복잡하며, 연산량을 줄이기 위해 초기 층에서 feature map의 공간 해상도를 더 공격적으로 줄인다. 4.5절에서 보이듯이, 단일 네트워크 분류 정확도 측면에서 우리의 모델은 Szegedy et al. (2014)의 모델을 능가한다.

---
### 내용
- 실험 목표 : 킾이를 증가시킴으로서 발생하는 개선 효과 측정
#### 2.1. Architecture
- input : 224x224 RGB
	- 전처리 : 훈련 세트에서 계산된 평균 RGB값을 각 픽셀에서 뺌(AlexNet과 동일)
- Conv : 2d, 3x3 필터, stride = 1, padding = 1
	- 1x1필터도 사용 : 선형 변환
- max-polling : 5회(conv뒤에 항상 배치 x), 2x2필터, stride = 2
- FC : 3개
- ReLU를 활성화 함수로 사용
	- 하나를 제외하고 LRN(AlexNet의 technique) -> 성능향상에 기여하지 않음
![VGG Architecture](/assets/img/posts/vcg/Pasted_image_20250901131049.png)

#### 포인트

| 항목      | 내용                                          |
| --------- | --------------------------------------------- |
| 데이터셋  | ImageNet (ILSVRC)                             |
| 입력 크기 | 224×224 RGB                                   |
| 모델 구조 | 3×3 conv (stride=1, padding=1), 1×1 conv 일부 |
| FC 구조   | 4096 → 4096 → 1000, 마지막 soft-max           |
| 활성 함수 | 모든 은닉층에 ReLU                            |
| 정규화    | 대부분 LRN 미사용 (메모리, 속도 문제)         |
#### 2.2. Configurations
- 층 너비는 64에서 시작해 풀링 때마다 2배씩 증가해 512까지 도달한다. A는 11층, E는 19층 구조다. 흥미롭게도 깊이가 깊어져도 파라미터 수는 얕지만 큰 필터를 쓰는 네트워크보다 적을 수 있다.
![VGG Configurations](/assets/img/posts/vcg/Pasted%20image%2020250901151759.png)
#### 포인트

| 항목        | 내용                                  |
| ----------- | ------------------------------------- |
| 네트워크 A  | 11층 (8 conv + 3 FC)                  |
| 네트워크 E  | 19층 (16 conv + 3 FC)                 |
| 채널 수     | 64에서 시작, 풀링마다 2배, 최종 512   |
| 파라미터 수 | A–E: 133M ~ 144M, 기존보다 효율적     |
| 비교 대상   | OverFeat (Sermanet et al., 2014) 144M |
#### 2.3. Discussion
- 차이점 : 3x3커널 stride = 1사용
- 또한 **한번에 큰 커널 7x7, 5x5를 사용하지 않고, 3x3을 쌓아서 사용**
	-  **효과 : 3x3커널 2장사용시 5x5커널을 사용하는 효과와 동일,  3장 사용시 7x7커널을 사용하는 효과와 동일**
	- 장점
		- **비선형함수가 3회 사용되어, 판별력이 더 좋아진다**
		- 입력과 출력이 모두 C 채널일 때, 세 층의 3×3 합성곱 스택은 3(3^2C^2) = 27C^2개의 가중치를 가지지만, 단일 7×7 층은 7^2C^2 = 49C^2개로 **파라미터 수가 줄어든다** 
		- **일종의 정규화**로 작용된다
- 1x1커널 사용 : 비선형성 증가 효과(활성화 함수에 의해)

#### 포인트

| 항목           | 내용                                     |
| -------------- | ---------------------------------------- |
| 비교 대상      | AlexNet(11×11), Zeiler(7×7), OverFeat 등 |
| VGG 접근       | 전 층에서 3×3 conv 사용                  |
| 장점 1         | 비선형성(예: 3개 ReLU vs 1개) 증가       |
| 장점 2         | 파라미터 감소 (27C^2 vs 49C^2, 81% 절약) |
| 장점 3         | 정규화 효과 (큰 필터를 작은 필터로 분해) |
| 1×1 conv       | receptive field 유지, 비선형성 추가      |
| GoogLeNet 비교 | 더 깊지만 복잡, VGG는 단순·성능 우수     |

---
### 📚 3. Classification Framework
### 번역
2장에서 모델의 구조와 변형을 설명한 데 이어, 3장에서는 **훈련(Training)과 평가(Testing) 프로토콜**을 다룬다. 학습률, 배치 크기, 정규화, 초기화, 데이터 증강, 테스트 시 방법론 등이 포함된다.
#### 3.1. Training
ConvNet 학습 절차는 일반적으로 Krizhevsky et al. (2012)을 따른다(단, 이후 설명할 다중 스케일 학습 이미지를 이용한 입력 crop 샘플링은 제외). 즉, 학습은 모멘텀을 사용한 미니배치 경사하강법(mini-batch gradient descent, 역전파(LeCun et al., 1989) 기반)을 통해 다항 로지스틱 회귀(multinomial logistic regression) 목적함수를 최적화하는 방식으로 수행된다. 배치 크기는 256, 모멘텀은 0.9로 설정되었다. 학습은 가중치 감쇠(weight decay, L2 패널티 계수 5·10^−4)와 첫 두 개의 완전연결 층에 대한 dropout 정규화(dropout 비율 0.5)로 정규화되었다. 초기 학습률은 10^−2로 설정되었으며, 검증 세트 정확도가 향상되지 않을 때마다 10배씩 감소되었다. 총 세 번 감소시켰고, 370K iteration(74 epoch)에서 학습을 종료했다. Krizhevsky et al. (2012)에 비해 매개변수 수와 깊이가 더 많음에도 불구하고, (a) 깊이와 작은 합성곱 필터가 내재적 정규화를 제공했으며, (b) 일부 층을 사전 초기화(pre-initialisation)했기 때문에 오히려 더 적은 epoch으로 수렴했다고 추측한다.

네트워크 가중치 초기화는 중요하다. 초기화가 잘못되면 기울기 불안정성으로 학습이 멈출 수 있기 때문이다. 이를 피하기 위해, 먼저 얕아서 무작위 초기화로도 학습 가능한 구성 A(Table 1)를 학습했다. 이후 더 깊은 아키텍처 학습 시, 처음 네 개 합성곱 층과 마지막 세 개 FC 층은 네트워크 A의 가중치로 초기화하고, 중간 층들은 무작위로 초기화했다. 사전 초기화된 층도 학습 중에 변하도록 학습률을 낮추지 않았다. 무작위 초기화의 경우 평균 0, 분산 10^−2인 정규분포에서 가중치를 샘플링했고, bias는 0으로 초기화했다. 논문 제출 이후 Glorot & Bengio (2010)의 무작위 초기화 방법을 사용하면 사전 학습 없이도 초기화가 가능함을 확인했다.

ConvNet 입력 크기 224×224 이미지를 얻기 위해, 훈련 이미지를 리스케일한 뒤 무작위로 crop을 잘라내어 사용했다(SGD iteration당 이미지당 1 crop). 데이터 증강으로는 무작위 좌우 반전(horizontal flipping)과 무작위 RGB 색상 변화(Krizhevsky et al., 2012)를 적용했다. 훈련 이미지 리스케일링은 아래와 같다.

###### **훈련 이미지 크기**: 
S를 리스케일된 훈련 이미지의 가장 짧은 변 길이라고 할 때(이를 학습 스케일이라 부른다), ConvNet 입력 crop 크기는 224×224로 고정되어 있으나 S는 224 이상 아무 값이나 가질 수 있다. S=224이면 crop은 이미지의 가장 짧은 변 전체를 포함하고, S≫224이면 crop은 작은 객체나 객체의 일부만 포함한다.

S를 설정하는 두 가지 접근을 고려한다. 첫째는 S를 고정하는 단일 스케일 학습(single-scale training)이다. 이 경우 crop 내 콘텐츠는 여전히 다중 스케일 정보를 나타낼 수 있다. 우리는 S=256(선행연구에서 널리 사용)과 S=384 두 가지 고정 스케일에서 학습을 수행했다. S=256으로 학습한 뒤, S=384 모델은 이를 사전 학습 가중치로 초기화하고 학습률을 10^−3으로 줄여 학습 시간을 단축했다.

둘째는 다중 스케일 학습(multi-scale training)으로, 각 훈련 이미지를 [S_min, S_max] 범위에서 무작위로 샘플링된 S로 리스케일한다(여기서는 S_min=256, S_max=512). 이는 객체 크기가 다양한 점을 고려하는 데 유리하며, 스케일 지터링(scale jittering)에 의한 데이터 증강으로도 볼 수 있다. 속도상의 이유로, 다중 스케일 모델은 동일한 구성의 S=384 단일 스케일 사전 학습 모델을 fine-tuning하여 학습했다.
#### 3.2. Testing
테스트 시, 학습된 ConvNet과 입력 이미지가 주어지면 다음과 같이 분류가 이루어진다. 먼저 이미지를 사전 정의된 최소 변 길이 Q로 등비적으로 리스케일한다(Q를 테스트 스케일이라 한다). Q는 반드시 학습 스케일 S와 같을 필요는 없다(4장에서 보이듯이, 각 S에 대해 여러 Q를 사용하는 것이 성능 향상으로 이어진다). 그 후 네트워크를 리스케일된 테스트 이미지 전체에 대해 조밀하게(densely) 적용하는데, 이는 (Sermanet et al., 2014)와 유사하다. 구체적으로는, 완전연결 층들을 합성곱 층으로 변환한다(첫 번째 FC 층은 7×7 conv 층으로, 마지막 두 개의 FC 층은 1×1 conv 층으로 변환). 이렇게 얻어진 완전합성곱 네트워크는 전체(잘리지 않은) 이미지에 적용된다. 결과는 클래스 수와 동일한 채널을 갖는 클래스 점수 맵이며, 이는 입력 이미지 크기에 따라 공간 해상도가 달라진다. 마지막으로, 이미지에 대한 고정 크기 클래스 점수 벡터를 얻기 위해 클래스 점수 맵을 공간적으로 평균(sum-pooling)한다. 또한 테스트셋 증강을 위해 이미지를 좌우 반전하고, 원본과 반전된 이미지의 soft-max 클래스 posterior를 평균내어 최종 점수를 얻는다.

완전합성곱 네트워크를 전체 이미지에 적용하기 때문에, 테스트 시 여러 crop을 샘플링할 필요가 없다(Krizhevsky et al., 2012). 여러 crop은 비효율적이며 crop마다 네트워크를 다시 계산해야 한다. 그러나 Szegedy et al. (2014)처럼 많은 crop을 사용하는 경우 더 세밀한 입력 이미지 샘플링이 이루어져 정확도가 개선될 수 있다. 또한 multi-crop 평가와 dense 평가가 상호 보완적인데, 이는 합성곱 경계 조건이 다르기 때문이다. crop에 ConvNet을 적용하면 합성곱 feature map은 0으로 패딩되지만, dense 평가에서는 동일한 crop의 패딩이 이미지의 인접 부분에서 자연스럽게 유도된다(합성곱과 pooling 모두로 인해). 이는 전체 네트워크의 수용영역을 크게 확장시켜 더 많은 컨텍스트를 포착하게 한다. 실제로는 multi-crop이 계산 시간이 크게 늘어나 정확도 개선 대비 효율성이 낮다고 보지만, 참고로 우리는 각 스케일당 50개 crop(5×5 격자와 좌우 반전 2개)을 사용해 총 150 crop(3개 스케일)으로 네트워크를 평가했으며, 이는 Szegedy et al. (2014)가 4개 스케일에서 144 crop을 사용한 것과 유사하다.
#### 3.3. 구현 디테일
우리의 구현은 공개된 C++ Caffe 툴박스(Jia, 2013, 2013년 12월 fork)에서 파생되었으나, 다수의 중요한 수정 사항을 포함한다. 이로써 단일 시스템에 설치된 다중 GPU에서 훈련과 평가를 수행할 수 있으며, 다중 스케일에서 잘리지 않은(full-size) 이미지를 학습 및 평가할 수 있다(앞서 설명한 바와 같이). 다중 GPU 학습은 데이터 병렬성(data parallelism)을 활용하며, 각 학습 배치를 여러 GPU 배치로 분할하여 각 GPU에서 병렬로 처리한다. GPU 배치의 gradient가 계산된 후 평균되어 전체 배치의 gradient를 얻는다. gradient 계산은 GPU 간 동기적으로 이루어지므로 단일 GPU에서 학습한 결과와 완전히 동일하다.
최근 Krizhevsky (2014)는 ConvNet 학습을 가속하기 위한 더 정교한 방법을 제안했는데, 이는 네트워크의 서로 다른 층에 대해 모델 병렬성과 데이터 병렬성을 결합한다. 그러나 우리는 개념적으로 훨씬 단순한 방식이 이미 단일 GPU 대비 4-GPU 시스템에서 약 3.75배의 속도 향상을 제공함을 확인했다. NVIDIA Titan Black GPU 4개를 장착한 시스템에서, 단일 네트워크를 학습하는 데 아키텍처에 따라 2–3주가 소요되었다.

### 내용
### 3. Classification Framework

#### 3.1. Training
- 모멘텀을 활용한 미니배치 경사하강법(m = 0.9, batch = 256)
- 정규화 : L2
- dropout = 0.5(첫 두개의 fc layer)
- Loss : multinomial logistic regression
- 학습률 : 임의 정의(개선 x 10배 감소)
- 총 74에폭
- 무거움에도 불구하고 커널 쌓기에 대한 정규화 + 일부 층 사전 초기화로 인해 더 적은 시간 소요
- 초기화 문제를 해결하기 위하여, 얕아서 무작위 초기화로도 학습가능한 A를 학습
	- 더 깊은 네트워크 : 처음 4개의 CNN layer과 마지막 fc layer들을 A의 네트워크 가중치로 초기화, 나머지는 무작위 초기화(학습률 감쇠 X)
	- 무작위 초기화의 경우 평균 0, 분산 $10^{-2}$인 정규분포에서 가중치를 샘플링했고, bias는 0으로 초기화

> [!NOTE]
> **세이비어 초기화(Xavier Initialization)**
> 
> **신호(활성화 값)가 네트워크의 여러 층을 통과하더라도 그 분산(크기)을 일정하게 유지하여, 기울기 소실(Vanishing Gradient)이나 기울기 폭주(Exploding Gradient)** 문제를 방지하고 안정적인 학습을 가능하게하는 것
> 
> - **효과**: 이 방법을 사용하면 층이 깊어져도 신호가 안정적으로 전달되어, 학습 초기 단계가 원활해지고 모델의 수렴 속도가 빨라집니다.
> - **한계**: 세이비어 초기화는 주로 **Sigmoid**나 **Tanh**와 같은 활성화 함수를 사용할 때 효과적입니다. 왜냐하면 이 함수들은 중앙 부분이 선형에 가깝다는 가정 하에 유도되었기 때문입니다. **ReLU** 활성화 함수와 함께 사용하면, 활성화 값의 절반이 0이 되면서 분산이 줄어들어 다시 기울기 소실 문제가 발생할 수 있다
> 
> 참고: https://at0z.tistory.com/35

* 데이터 증강 : 리스케일+무작위 crop,  색변환과 뒤집기는AlexNet의 증강기법과 동일

##### 훈련데이터
- 224보다 큰 S를 설정하고, 두가지 접근을 고려
	- 단일스케일 : 연구 내에서는 256 -> 384를 사용 작은 S로 사전학습 후 384를 학습(학습률 $10^{-3}$)
	- 다중스케일 : 각 훈련이미지를 [S_min, S_max]로 사용하여, 무작위로 샘플된 S로 리스케일을 진행으로 스케일 지터링(scale jittering)에 의한 데이터 증강으로도 볼 수 있다.

> [!NOTE]
> **스케일지터링(Scale Jittering)**
> 
> - **스케일 (Scale)**: 이미지의 크기 또는 배율
> - **지터링 (Jittering)**: 무언가를 미세하고 불규칙하게 흔드는 것
> 
> 즉, **'이미지 크기를 조금씩 무작위로 바꾸는 것'**을 의미해요. 예를 들어, 훈련 과정에서 동일한 고양이 사진을 보여줄 때마다 크기를 90%, 115%, 85%, 120% 등으로 계속 바꿔서 모델에 입력하는 방식입니다.

#### 포인트

| 항목        | 내용                                           |
| ----------- | ---------------------------------------------- |
| Loss        | 다항 로지스틱 회귀 (Softmax cross-entropy)     |
| Optimizer   | 미니배치 SGD + Momentum(0.9)                   |
| 정규화      | Weight decay(5e−4), Dropout(0.5, FC층)         |
| 학습률      | 초기 0.01, 개선 정체 시 10배 감소, 총 3회 감소 |
| 초기화      | 얕은 A로 학습 후, deeper 모델 일부 층에 재사용 |
| 데이터 증강 | Random crop, flip, RGB shift, scale jittering  |
| 스케일 설정 | S=256, 384 (고정) / [256,512] (multi-scale)    |
#### 3.2. Testing
- Q(테스트 스케일, 등비적, S와 동일할 필요 없음 - 오히려 성능 향상)
- fc layer들을 conv로 변환(7x7 -> 1x1 -> 1x1)후 잘리지 않은 이미지에 적용
- output은 class수와 동일한 채널수를 가지는 클래스 점수 맵, 입력 이미지에 따라 공간해상도가 달라진다
	- 이미지에 대한 고정 class 벡터를 얻기 위해, 클래스 맵을 공간적으로 평균(sum-pooling)
- 테스트셋 증강 : 좌우 반전 후 softmax클래스의 posterior을 내서 평균후 최종 점수

> [!NOTE]
> **FC Layer를 Conv Layer로 변환하는 원리**
> 
> ##### 가중치 유지는 어떻게?
> 
> - **기존 방식 (FC Layer):**
>     
>     1. `7x7x512` 크기의 피처 맵을 **일렬로 쭉 폅니다(flatten)**. 그러면 `25,088` (7 * 7 * 512) 크기의 벡터가 됩니다.
>         
>     2. 이 벡터에 `[25088, 4096]` 크기의 거대한 가중치 행렬을 곱합니다.
>         
>     3. 결과적으로 `4096` 크기의 벡터가 나옵니다.
>         
> - **변환 방식 (Conv Layer):**
>     
>     1. `[25088, 4096]` 크기의 FC 가중치 행렬을 **`[7, 7, 512, 4096]`** 크기의 합성곱 필터(커널)로 **형태를 바꿉니다(reshape)**.
>         
>     2. 이것은 곧, **크기가 `7x7`이고 입력 채널이 `512`인 필터가 `4096`개** 있다는 의미입니다.
>         
>     3. 이 `7x7x512` 크기의 필터 `4096`개를 `7x7x512` 입력 피처 맵에 적용합니다.
>         
>     4. 필터의 공간적 크기(`7x7`)가 입력 피처 맵의 공간적 크기(`7x7`)와 정확히 같기 때문에, 필터는 한 번만 연산되고 결과적으로 **`1x1x4096`** 크기의 출력 맵이 나옵니다.
>         
> 
> **핵심 포인트:** `25,088` 크기 벡터와 `[25088, 4096]` 행렬의 곱셈 연산은, `7x7x512` 피처 맵에 `7x7x512` 크기의 필터 `4096`개를 적용하는 합성곱 연산과 **수학적으로 완전히 동일합니다.** 가중치는 그대로 사용되며, 단지 연산 방식의 관점만 바뀐 것입니다.

- Conv로 바꾸기 때문에 테스트시 crop할 필요가 없으나, Szegedy et al. (2014)처럼 많은 crop을 사용하는 경우 더 세밀한 입력 이미지 샘플링이 이루어져 정확도가 개선될 수 있다.
- multi crop 평가와 dense 평가가 상호 보완적, 이는 합성곱 경계조건이 다르기 때문이다.
	- crop에 ConvNet을 적용하면 합성곱 feature map은 0으로 패딩되지만
	- dense 평가에서는 동일한 crop의 패딩이 이미지의 인접 부분에서 자연스럽게 유도된다. 이는 전체 네트워크의 수용영역을 크게 확장시켜 더 많은 컨텍스트를 포착
- 즉 conv로 바꾸어 전체 이미지를 dense하게 평가하면 multi crop보다 효율적임, 다만 multi-crop을 사용시 정확도를 조금 더 높일 수 있다.

> [!NOTE]
> **Multi-crop vs Dense 평가 방식 비교**
> 
> ### Multi-crop 평가 (Krizhevsky et al. 방식)
> - 원본 이미지에서 여러 crop(예: 224×224)을 잘라냅니다.
> - 잘라낸 crop만 ConvNet에 입력합니다.
> - ConvNet 연산 과정에서, feature map의 경계 부분은 **0-padding**을 씁니다.
>     - 예: 3×3 conv라면, 바깥쪽 한 칸은 실제 픽셀이 아니라 0이 채워져서 연산됩니다.
> - 따라서 crop 주변(자른 영역 바깥)의 정보는 **아예 없는 것처럼 처리**됩니다.
> - 즉, crop 단위로 네트워크가 고립된 상태에서 동작합니다.
> ## 2. Dense 평가 (Sermanet 방식, VGG에서 활용)
> - FC layer를 conv로 변환해 **전체 이미지**를 한 번에 넣습니다.
> - 이때 네트워크는 **sliding window 방식**으로 전체 이미지를 훑으면서 지역적인 feature를 계산합니다.
> - 어떤 위치의 receptive field가 원래 crop 영역과 겹친다고 해도, 그 receptive field의 바깥쪽 픽셀은 **실제로 존재하는 이웃 픽셀**을 가져옵니다.
>     - 즉, crop을 따로 자른 경우엔 바깥이 “0”인데, dense 평가에선 바깥쪽이 “실제 다른 부분 픽셀”이 되는 거예요.
> ## 3. 차이가 의미하는 것
> - Multi-crop 평가: crop 바깥은 무조건 **0으로 가정** → 문맥(Context) 정보 손실
> - Dense 평가: crop 바깥은 **실제 이미지의 다른 부분으로 채워짐** → receptive field가 넓어져 더 많은 컨텍스트 활용 가능
> 즉, **같은 위치의 crop이라도 dense 평가에서는 추가적인 문맥 정보**를 활용할 수 있고, 이게 정확도 향상에 기여할 수 있다는 뜻입니다.

#### 포인트

| 항목                | 내용                                                    |
| ------------------- | ------------------------------------------------------- |
| 테스트 입력 Q       | 최소 변 길이 Q로 리스케일, 학습 스케일 S와 다를 수 있음 |
| 방법 1 (Dense)      | FC → conv 변환, 전체 이미지 평가, score map 평균        |
| 방법 2 (Multi-crop) | crop 여러 개 추출 후 평가, 보완적 효과 있음             |
| 장점/단점           | Dense: 효율적 / Multi-crop: 계산량 크지만 정확도 ↑      |
| 실험 설정           | VGG: 3개 스케일 × 50 crop = 150 crop 평가               |
#### 3.3. 구현 디테일

#### 포인트

| 항목           | 내용                                            |
| -------------- | ----------------------------------------------- |
| 구현 기반      | Caffe (2013년 12월 fork)                        |
| 주요 수정 사항 | 멀티 GPU 학습, 전체 이미지 멀티스케일 평가 지원 |
| 병렬화 방식    | 데이터 병렬 (batch 분할, gradient 평균, 동기식) |
| 속도 향상      | 4-GPU 사용 시 3.75배 가속                       |
| 하드웨어       | NVIDIA Titan Black ×4                           |
| 학습 시간      | 단일 네트워크 학습에 2–3주 (아키텍처 의존)      |

---
### 📚 4. Classification Experiments

### 번역
**데이터셋** 
본 장에서는 앞서 설명한 ConvNet 아키텍처들이 ILSVRC-2012 데이터셋에서 달성한 이미지 분류 결과를 제시한다(이 데이터셋은 ILSVRC 2012–2014 챌린지에 사용됨). 데이터셋은 1000개의 클래스를 포함하며, 세 부분으로 나뉜다: 학습 세트(130만 장), 검증 세트(5만 장), 테스트 세트(10만 장, 라벨은 비공개). 분류 성능은 두 가지 지표로 평가된다: top-1 에러와 top-5 에러. top-1 에러는 다중 클래스 분류 오차(즉, 잘못 분류된 이미지의 비율)를 의미하며, top-5 에러는 ILSVRC의 주요 평가 기준으로, 정답 클래스가 예측된 상위 5개 클래스 밖에 있을 확률을 의미한다.
대부분의 실험에서는 검증 세트를 테스트 세트로 사용하였다. 일부 실험은 실제 테스트 세트에서도 수행되어, ILSVRC-2014 대회에 “VGG” 팀 엔트리로 제출되었다(Russakovsky et al., 2014).
#### 4.1. SINGLE SCALE EVALUATION
우리는 먼저 2.2절에서 설명한 층 구성을 가진 개별 ConvNet 모델의 성능을 단일 스케일에서 평가한다. 테스트 이미지 크기는 고정된 S의 경우 Q = S로 설정했고, jittered S ∈ [S_min, S_max]의 경우 Q = 0.5(S_min + S_max)로 설정했다. 결과는 표 3에 제시되어 있다.
첫째, Local Response Normalisation(LRN, A-LRN 네트워크 사용)은 정규화 층이 없는 모델 A보다 성능을 개선하지 못한다. 따라서 더 깊은 아키텍처(B–E)에서는 정규화를 사용하지 않았다.

둘째, ConvNet 깊이가 증가할수록 분류 오류가 감소한다: A의 11층에서 E의 19층까지. 특히 동일한 깊이임에도 불구하고 1×1 conv 층 세 개를 포함하는 구성 C는 네트워크 전체에서 3×3 conv 층을 사용하는 구성 D보다 성능이 낮았다. 이는 추가적인 비선형성이 도움이 되긴 하지만(C가 B보다 성능이 나음), 공간적 문맥을 포착하기 위해 비자명한 수용영역(conv 필터)을 사용하는 것도 중요함을 의미한다(D가 C보다 성능이 나음). 아키텍처의 오류율은 깊이가 19층에 도달했을 때 포화되지만, 더 큰 데이터셋에서는 더 깊은 모델이 유용할 수 있다. 또한 우리는 net B를 얕은 네트워크와 비교했는데, 이는 B의 각 3×3 conv 층 쌍을 단일 5×5 conv 층으로 대체한 것이다(2.3절에서 설명한 것처럼 동일한 수용영역). 얕은 네트워크의 top-1 오류율은 B보다 7% 더 높았는데(center crop 기준), 이는 작은 필터를 가진 깊은 네트워크가 큰 필터를 가진 얕은 네트워크보다 성능이 우수함을 확인시켜준다.

마지막으로, 학습 시 scale jittering(S ∈ [256; 512])은 고정된 가장 짧은 변 크기(S = 256 또는 S = 384)로 학습한 경우보다 결과가 현저히 개선되며, 테스트에서는 단일 스케일만 사용하더라도 효과적이었다. 이는 scale jittering을 통한 학습 데이터셋 증강이 다중 스케일 이미지 통계 포착에 실제로 도움이 됨을 확인시켜준다.
#### 4.2. MULTI-SCALE EVALUATION
단일 스케일에서 ConvNet 모델들을 평가한 후, 이제 테스트 시의 scale jittering 효과를 평가한다. 이는 하나의 모델을 여러 리스케일된 버전의 테스트 이미지(Q 값이 다름)에 적용한 후, 결과 클래스 posterior를 평균내는 방식이다. 학습 스케일과 테스트 스케일 간 불일치가 클 경우 성능 저하가 발생하기 때문에, 고정된 S로 학습된 모델은 학습 스케일과 가까운 세 가지 테스트 크기에서 평가되었다: Q = {S − 32, S, S + 32}. 한편, 학습 시 scale jittering을 적용한 모델은 테스트 시 더 넓은 스케일 범위에 적용할 수 있으므로, 변수 S ∈ [S_min; S_max]로 학습된 모델은 더 넓은 크기 범위에서 평가되었다: Q = {S_min, 0.5(S_min + S_max), S_max}.
표 4에 제시된 결과는, 테스트 시 scale jittering이 동일 모델을 단일 스케일에서 평가한 것(Table 3)보다 성능을 향상시킴을 보여준다. 앞서와 같이 가장 깊은 구성(D와 E)이 가장 좋은 성능을 보였으며, scale jittering은 고정된 최소 변 S로 학습한 경우보다 우수하다. 우리의 단일 네트워크 기준 검증 세트에서 최고의 성능은 top-1/top-5 에러 24.8%/7.5% (표 4에서 굵게 표시). 테스트 세트에서는 구성 E가 top-5 에러 7.3%를 달성했다.
#### 4.3 MULTI-CROP EVALUATION
표 5에서는 dense ConvNet 평가와 multi-crop 평가를 비교한다(자세한 내용은 3.2절 참조). 또한 두 평가 기법의 soft-max 출력을 평균하여 상호 보완성을 평가한다. 결과적으로, 다중 crop을 사용하는 것이 dense 평가보다 약간 더 우수하며, 두 접근 방식을 결합하면 각각의 성능을 초과하는 결과를 낸다. 앞서 언급했듯이, 이는 합성곱 경계 조건을 다루는 방식의 차이 때문이라고 가정한다.
#### 4.4 CONVNET FUSION
지금까지는 개별 ConvNet 모델의 성능을 평가했다. 이번 실험에서는 여러 모델의 soft-max 클래스 posterior를 평균하여 출력을 결합한다. 이는 모델들의 상호 보완성 덕분에 성능을 향상시키며, 2012년(Krizhevsky et al., 2012)과 2013년(Zeiler & Fergus, 2013; Sermanet et al., 2014) ILSVRC 상위 제출작들에서도 사용되었다.
결과는 표 6에 제시되어 있다. ILSVRC 제출 시점에는 단일 스케일 네트워크와 FC 층만 fine-tuning한 multi-scale 모델 D만 학습된 상태였다. 이들 7개 네트워크 앙상블은 ILSVRC 테스트 에러 7.3%를 기록했다. 제출 이후, 가장 성능이 좋은 두 multi-scale 모델(D와 E)만을 결합한 앙상블을 고려했는데, dense 평가에서는 7.0%, dense와 multi-crop 평가를 결합하면 6.8%로 테스트 에러가 줄었다. 참고로, 단일 모델 기준 최고 성능은 모델 E로 7.1% 에러(Table 5).
#### 4.5 Comparison with the State of the Art
마지막으로, 우리의 결과를 표 7에서 최첨단(state of the art)과 비교한다. ILSVRC-2014 분류 과제(Russakovsky et al., 2014)에서, “VGG” 팀은 7개 모델 앙상블로 7.3% 테스트 에러를 기록해 2위를 차지했다. 제출 이후, 단 2개 모델 앙상블로 에러율을 6.8%까지 낮추었다.
표 7에서 보이듯, 우리의 매우 깊은 ConvNet은 ILSVRC-2012 및 ILSVRC-2013 대회에서 최고 성능을 기록했던 이전 세대 모델들을 크게 능가한다. 우리의 결과는 분류 과제 우승 모델 GoogLeNet(6.7% 에러)과도 경쟁 가능하며, 외부 데이터를 사용한 경우 11.2%, 사용하지 않은 경우 11.7%를 기록한 ILSVRC-2013 우승작 Clarifai보다 훨씬 뛰어나다. 특히 주목할 점은, 대부분의 ILSVRC 제출작들이 여러 모델을 결합한 데 반해, 우리는 단 2개 모델만 결합하여 최고 결과를 달성했다는 것이다. 단일 네트워크 성능 기준으로는, 우리의 아키텍처가 7.0% 테스트 에러를 기록해 단일 GoogLeNet보다 0.9% 더 우수하다. 중요한 점은, 우리는 LeCun et al. (1989)의 전통적인 ConvNet 아키텍처에서 벗어나지 않았으며, 단지 깊이를 크게 확장함으로써 성능을 향상시켰다는 것이다.
### 내용
- 데이터셋은 top-1, top-5 에러율로 측정

#### 포인트

| 항목        | 내용                                   |
| ----------- | -------------------------------------- |
| 데이터셋    | ILSVRC-2012 (1000 클래스)              |
| 학습 세트   | 1.3M 이미지                            |
| 검증 세트   | 50K 이미지                             |
| 테스트 세트 | 100K 이미지 (라벨 비공개)              |
| 평가 지표   | Top-1 error, Top-5 error               |
| 제출        | 일부 실험 결과는 ILSVRC-2014 공식 제출 |
#### 4.1. SINGLE SCALE EVALUATION
![VCG Single Scale Results](/assets/img/posts/vcg/Pasted%20image%2020250901203523.png)
* 단일 스케일 평가 결과, **깊이가 깊을수록 에러율 감소** 효과가 확인
* 하지만 단순히 비선형성만 늘린 1×1 conv 구성(C)은 3×3 conv를 쌓은 구성(D)보다 성능 하락
* scale jittering**이 중요한 데이터 증강 기법임을 실험으로 입증
#### 포인트

| 항목           | 내용                                         |
| -------------- | -------------------------------------------- |
| LRN 효과       | 성능 개선 없음, 오히려 불필요                |
| 깊이 증가 효과 | 11층(A) → 19층(E)로 갈수록 오류율 감소       |
| C vs D 비교    | 1×1 conv만 추가(C)보다 3×3 conv(D)가 우수    |
| 얕은 vs 깊은   | 깊고 작은 필터 > 얕고 큰 필터 (성능 7% 차이) |
| 데이터 증강    | Scale jittering이 성능 향상에 큰 기여        |
#### 4.2. MULTI-SCALE EVALUATION
![VGG Multi-Scale Results](/assets/img/posts/vcg/Pasted%20image%2020250901203959.png)
* 이 절은 **테스트 시 scale jittering**
* 학습 스케일 근처의 여러 Q로 평가하거나, 학습 자체를 jittering으로 했을 경우 더 넓은 범위에서 테스트할 수 있다. 결과적으로, 가장 깊은 모델(D, E)이 가장 좋은 성능을 내며, 단일 네트워크 기준으로 **24.8%/7.5% (top-1/top-5)**를 달성했다. 테스트 세트에서는 E가 7.3% top-5 에러를 기록
#### 포인트

| 항목                 | 내용                                     |
| -------------------- | ---------------------------------------- |
| 테스트 jittering     | 여러 Q에서 평가 후 posterior 평균        |
| 고정 S 모델 평가     | Q = {S−32, S, S+32}                      |
| jittered S 모델 평가 | Q = {S_min, 0.5(S_min+S_max), S_max}     |
| 최고 성능            | 단일 모델, Val: 24.8% top-1 / 7.5% top-5 |
| Test 성능            | Net-E, top-5 error 7.3%                  |
#### 4.3 MULTI-CROP EVALUATION
![VGG Multi-Crop Results](/assets/img/posts/vcg/Pasted%20image%2020250901204021.png)
* multi-crop 평가가 dense 평가보다 약간 낫지만, 두 방식을 결합하면 가장 좋은 성능을 보인다. 이는 **경계 조건 차이** 때문으로, crop 기반은 0 패딩을 사용하지만 dense는 주변 픽셀 정보를 활용해 더 넓은 문맥을 반영한다. 따라서 서로 보완적이다.

#### 포인트

| 항목            | 내용                                              |
| --------------- | ------------------------------------------------- |
| Dense 평가      | 전체 이미지 적용 (효율적)                         |
| Multi-crop 평가 | 여러 crop 평가 (정확도 ↑, 계산량 ↑)               |
| 결합 효과       | Dense + Multi-crop > 각각 단독 사용               |
| 원인 추정       | 합성곱 경계 조건 차이 (zero padding vs 주변 정보) |
#### 4.4 CONVNET FUSION
![VGG ConvNet Fusion Results](/assets/img/posts/vcg/Pasted%20image%2020250901204230.png)
* 여러 모델을 결합(앙상블)하면 성능 개선 
* VGGNet은 초기 7개 네트워크 앙상블로 7.3% 에러를 기록했으나, 이후 단 두 모델(D+E) 앙상블로도 더 낮은 **6.8% 에러**를 달성했다. 모**델 간 보완성이 크며, 다수의 모델이 꼭 필요하지 않음을 보여줌**

#### 포인트

| 항목           | 내용                                                |
| -------------- | --------------------------------------------------- |
| 방법           | 여러 모델 soft-max posterior 평균                   |
| 초기 앙상블    | 7개 모델 (단일·multi-scale 혼합), 7.3% error        |
| 개선된 앙상블  | 2개 모델 (D+E), dense: 7.0%, dense+multi-crop: 6.8% |
| 단일 모델 성능 | 모델 E, 7.1% error                                  |
| 의미           | 앙상블 보완성, 적은 모델로도 높은 성능 가능         |
#### 4.5 Comparison with the State of the Art
![VGG State-of-the-Art Comparison](/assets/img/posts/vcg/Pasted%20image%2020250901204356.png)
---
### 📚 A. Localisation

### 번역
#### 1
객체 위치 추정을 수행하기 위해, 우리는 마지막 완전연결 층이 클래스 점수가 아니라 경계 상자(bounding box) 위치를 예측하는 매우 깊은 ConvNet을 사용한다. 경계 상자는 중심 좌표, 너비, 높이를 저장하는 4차원 벡터로 표현된다. 경계 상자 예측은 모든 클래스에 대해 공유될 수도(single-class regression, SCR (Sermanet et al., 2014)), 클래스별로 개별적일 수도 있다(per-class regression, PCR). 전자의 경우 마지막 층은 4차원이고, 후자의 경우 데이터셋에 1000 클래스가 있으므로 4000차원이 된다. 마지막 경계 상자 예측 층을 제외하면, 분류 과제에서 가장 성능이 좋았던 ConvNet 아키텍처 D(표 1, 16개의 가중치 층)를 사용한다.

**훈련.** 
localisation ConvNet 훈련은 분류 ConvNet 훈련(3.1절)과 유사하다. 주요 차이는 로지스틱 회귀 목적함수 대신 유클리드 손실(Euclidean loss)을 사용하여 예측된 경계 상자 매개변수와 정답의 차이를 벌점화한다는 점이다. 우리는 두 개의 localisation 모델을 단일 스케일(S=256, S=384)에서 각각 학습했다(시간 제약으로 인해 ILSVRC-2014 제출에서는 scale jittering을 사용하지 않음). 훈련은 동일한 스케일에서 학습된 분류 모델로 초기화되었으며, 초기 학습률은 10^−3으로 설정했다. 또한 Sermanet et al. (2014)처럼 모든 층을 fine-tuning하는 방법과 첫 두 개 FC 층만 fine-tuning하는 방법을 모두 탐색했다. 마지막 FC 층은 무작위로 초기화하고 처음부터 학습했다.

**테스트.**
두 가지 테스트 프로토콜을 고려했다. 첫 번째는 검증 세트에서 네트워크 수정의 효과를 비교하기 위한 것으로, 분류 오류를 배제하기 위해 정답 클래스에 대해서만 경계 상자를 예측한다. 이때 경계 상자는 이미지 중앙 crop에 네트워크를 적용해 얻는다.

두 번째, 완전한 테스트 절차는 분류 과제(3.2절)와 유사하게 localisation ConvNet을 이미지 전체에 조밀하게 적용한다. 차이점은 마지막 FC 층 출력이 클래스 점수 맵이 아니라 경계 상자 예측 집합이라는 점이다. 최종 예측을 만들기 위해 우리는 Sermanet et al. (2014)의 greedy 병합 절차를 사용했다. 이는 먼저 공간적으로 가까운 예측들을 병합(좌표 평균)하고, 이후 분류 ConvNet에서 얻은 클래스 점수를 기반으로 랭킹을 매긴다. 여러 localisation ConvNet을 사용하는 경우, 먼저 각 ConvNet의 예측을 합쳐(union) 병합 절차를 적용했다. Sermanet et al. (2014)의 multiple pooling offsets 기법(경계 상자 예측의 공간 해상도를 높이는 방법)은 사용하지 않았다.
#### 2
이번 절에서는 먼저 첫 번째 테스트 프로토콜을 이용해 가장 성능이 좋은 localisation 설정을 결정하고, 이후 두 번째 프로토콜을 사용한 완전한 시나리오에서 이를 평가한다. localisation 오류는 ILSVRC 기준(Russakovsky et al., 2014)에 따라 측정되며, 예측된 경계 상자가 정답 경계 상자와의 교집합-합집합 비율(IoU)이 0.5 이상일 때 정답으로 간주된다.
설정 비교. 표 8에서 보듯이, per-class regression (PCR)이 클래스 비특정(single-class) 회귀(SCR)보다 우수하다. 이는 PCR이 SCR보다 성능이 낮았던 Sermanet et al. (2014)의 결과와는 다르다. 또한 localisation 과제에서 모든 층을 fine-tuning하는 것이, Sermanet et al. (2014)처럼 FC 층만 fine-tuning하는 것보다 눈에 띄게 성능이 좋았다. 이 실험에서는 최소 이미지 변을 S=384로 설정했다. S=256의 결과도 동일한 경향을 보였으나, 간결성을 위해 생략한다.

완전한 평가. 최적의 설정(PCR, 모든 층 fine-tuning)을 찾은 후, 이를 완전한 시나리오에 적용했다. 여기서는 4.5절의 최고 성능 분류 시스템을 사용해 top-5 클래스 라벨을 예측하고, 여러 조밀하게 계산된 bounding box 예측들을 Sermanet et al. (2014)의 방법으로 병합한다. 표 9에서 보듯이, localisation ConvNet을 전체 이미지에 적용하는 것은 중앙 crop만 사용한 것(표 8)보다 성능을 크게 개선한다. 이는 정답 라벨이 아닌 예측된 top-5 라벨을 사용했음에도 불구하고 그렇다. 분류 과제(4절)와 유사하게, 여러 스케일에서 테스트하고 다수 네트워크의 예측을 결합하면 성능이 추가로 향상된다.

최첨단과의 비교. 우리의 최고 localisation 결과를 표 10에서 기존 최첨단과 비교했다. 테스트 에러 25.3%로, “VGG” 팀은 ILSVRC-2014 localisation 과제에서 우승했다(Russakovsky et al., 2014). 주목할 점은, 우리는 Overfeat (Sermanet et al., 2014, ILSVRC-2013 우승)보다 훨씬 더 좋은 성능을 달성했는데, 더 적은 스케일을 사용했고 그들의 해상도 향상 기법(resolution enhancement)도 사용하지 않았음에도 불구하고 그렇다. 만약 이 기법을 결합한다면 더 나은 성능을 얻을 수 있을 것이다. 이는 매우 깊은 ConvNet이 가져온 성능 향상을 보여준다 — 더 단순한 localisation 방법을 사용했음에도 더 강력한 표현 덕분에 더 나은 결과를 얻을 수 있었다.
### 내용
#### 1
- **마지막 clf.가 클래스 점수가 아니라, bounding box를 예측하는 convnet**을 사용
	- bounding box : 중심 좌표, 너비, 높이를 저장하는 4차원 vec.
	- 경계상자 예측은 공유될수도, 클래스별로 개별적일 수도 있다.
		- Signle-class reg., SCR : 마지막층 4차원
		- per-class reg., PCR : 마지막층 4차원 x 1000클래스
- Loss의 변경 : Logistic reg.에서 L2 loss으로 변경하여, 경계상자의 예측과 정답을 벌점화 진행
- 두 스케일에 대하여 학습
- 테스트시 최종 예측을 만들기 위해 우리는 Sermanet et al. (2014)의 greedy 병합 절차를 사용
	- 공간적으로 가까운 예측들을 병합(좌표 평균)하고, 이후 분류 ConvNet에서 얻은 클래스 점수를 기반으로 랭킹을 매긴다

#### 포인트

| 항목        | 내용                                             |
| ----------- | ------------------------------------------------ |
| 과제        | ILSVRC 2014 localisation (25.3% error, 우승)     |
| 아키텍처    | ConvNet-D (16층), 마지막 층 → bounding box 예측  |
| 레이블 방식 | SCR (공유, 4D) vs PCR (클래스별, 4000D)          |
| 손실 함수   | Euclidean loss                                   |
| 초기화      | 분류 모델 가중치 재사용, 마지막 FC 무작위 초기화 |
| 테스트 방식 | ① GT 클래스 중앙 crop, ② Dense + greedy merging  |
#### 2
* **최적 설정(PCR + all layers fine-tuning)**
* **깊은 네트워크 자체가 강력한 표현력**

#### 포인트

| 항목      | 내용                                           |
| --------- | ---------------------------------------------- |
| 평가 기준 | IoU ≥ 0.5 (ILSVRC)                             |
| 설정 비교 | PCR > SCR, All layers fine-tuning > FC-only    |
| 최적 설정 | PCR + 모든 층 fine-tuning                      |
| 전체 평가 | Dense 적용 + greedy merging + multi-scale      |
| 최종 성과 | 25.3% error, ILSVRC 2014 localisation 우승     |
| 비교      | VGG > OverFeat (더 단순 방법으로 더 나은 결과) |

---
### 📚 B. Generalisation of Very Deep Features

### 번역
앞 절들에서는 ILSVRC 데이터셋에서 매우 깊은 ConvNet의 학습과 평가를 다루었다. 이번 절에서는 ILSVRC에서 사전 학습된 ConvNet을 다른 더 작은 데이터셋의 특징 추출기(feature extractor)로 평가한다. 작은 데이터셋에서는 과적합(over-fitting) 문제로 인해 대규모 모델을 처음부터 학습하는 것이 불가능하기 때문이다. 최근 이러한 사용 사례에 대한 관심이 커졌는데(Zeiler & Fergus,![[Obsidian Vault 가상본]] 2013; Donahue et al., 2013; Razavian et al., 2014; Chatfield et al., 2014), ILSVRC에서 학습된 심층 이미지 표현이 다른 데이터셋에도 잘 일반화되며, 수작업으로 설계된 표현(hand-crafted representations)을 큰 차이로 능가한다는 것이 밝혀졌다. 이 연구 흐름에 따라, 우리는 우리의 모델이 기존 state-of-the-art 방법에서 사용된 얕은 모델보다 더 나은 성능을 내는지를 조사한다. 이 평가에서는 ILSVRC에서 가장 좋은 분류 성능을 낸 두 모델(4장에서 설명) — 구성 “Net-D”와 “Net-E”를 고려한다(이 두 모델은 공개됨).
ILSVRC에서 사전 학습된 ConvNet을 다른 데이터셋의 이미지 분류에 활용하기 위해, 마지막 FC 층(1000-way ILSVRC 분류 수행)을 제거하고,倒수 두 번째 층의 4096차원 활성값을 이미지 특징으로 사용한다. 이 특징은 여러 위치와 스케일에서 집계되며, L2 정규화 후 대상 데이터셋에서 학습된 선형 SVM 분류기에 입력된다. 단순화를 위해, 사전 학습된 ConvNet 가중치는 고정되며(fine-tuning 없음), 변하지 않는다.

특징 집계는 ILSVRC 평가 절차(3.2절)와 유사하게 수행된다. 즉, 이미지를 가장 짧은 변이 Q가 되도록 리스케일한 뒤, 네트워크를 이미지 평면 전체에 조밀하게 적용한다(모든 가중치 층을 합성곱으로 처리할 수 있으므로 가능하다). 이후 결과 feature map에 대해 전역 평균 풀링(global average pooling)을 수행해 4096차원 이미지 기술자를 얻는다. 이 기술자는 좌우 반전된 이미지의 기술자와 평균된다. 4.2절에서 보였듯, 다중 스케일 평가가 유리하기 때문에, 여러 Q에서 특징을 추출했다. 결과로 얻어진 multi-scale 특징은 스케일 간에 쌓거나(stack) 평균할 수 있다. stacking은 분류기가 다양한 스케일의 이미지 통계를 최적으로 결합하도록 학습할 수 있게 하지만, 기술자의 차원이 증가하는 비용이 따른다. 아래 실험에서 이 설계 선택을 다시 논의한다. 또한 두 네트워크(Net-D, Net-E)를 사용할 때 각각의 기술자를 쌓아 late fusion을 수행하는 것도 평가했다.

### 내용
마지막 1000-way FC는 제거하고 4096차원 벡터를 활용했다. 다중 스케일 특징은 평균(pooling)하거나 쌓기(stacking)로 결합할 수 있으며, 두 네트워크 특징도 late fusion이 가능하다. Fine-tuning은 하지 않았다.

#### 포인트

| 항목        | 내용                                            |
| ----------- | ----------------------------------------------- |
| 대상 문제   | 작은 데이터셋에서 사전 학습 모델 활용           |
| 사용 모델   | Net-D (16층), Net-E (19층), ILSVRC 사전 학습    |
| 특징 추출   | FC-4096 차원 벡터, 마지막 FC 제거               |
| 분류기      | Linear SVM                                      |
| 스케일 처리 | Multi-scale Q, pooling vs stacking              |
| 결합 방식   | Net-D & Net-E late fusion (descriptor stacking) |
