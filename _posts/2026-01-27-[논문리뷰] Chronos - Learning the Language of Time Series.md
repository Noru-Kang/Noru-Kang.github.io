---
title: Chronos - Learning the Language of Time Series
date: 2026-01-27 14:00:00 +0900
categories:
  - AI-ML-DL
  - etc.
tags:
  - time-series
  - TS_model
math : true
---
### 🔗 출처
> https://arxiv.org/abs/2403.07815

---
## 🗓️ 요약
### 📌 3줄 요약

1. 시계열 값을 스케일링 및 양자화하여 고정된 어휘로 토큰화하고, 기존 Transformer 기반 언어 모델 아키텍처를 최소한의 수정만으로 이 토큰화된 시계열에 대해 CE를 사용하여 학습시키는 pretrained 모델
2. 공개된 대규모 데이터셋과, Gaussian Process기반의 합청 데이터셋(KernelSYnth), TSMixup증강으로 사전학습, TS의 특징을 사용하지 않고, autogressive sampling을 통해 확률적 예측을 수행
3. Chronos는 성능이 잘 나오고, zero-shot에 특화됨

### 📍요약

"TS"데이터를 언어 모델이 이해할 수 있는 "언어"로 변환하여, 기존의 트랜스포머 기반 LM 아키텍처를 시계열 예측에 적용

#### 1. 핵심 방법론
 **시계열 데이터의 토큰화, 언어 모델 재활용**
##### 1.1. 시계열 토큰화
 실수 값을 가지는 시계열을 유한한 어휘의 이산적인 토큰으로 매핑
- **Scaling** : **Mean Scailing** 사용, 시계열의 각 엔트리를 context내의 절대값의 평균으로 나눔 $\tilde{x}_i = (x_i - m) / s$ 여기서 m=0 이고, $s = C^{-1} \sum_{j=1}^{C} |x_j|$ 임. 이는 시계열의 0을 보호하는 이점을 가짐
- **Quantization** : 스케일링된 실수 값 $\tilde{x}_i$을 이산적인 토큰 ID로 변환함. B개의 칸(Bin) 중심 $c_1 < \ldots < c_B$ 와 B-1개의 $b_i$를 설정, $$
q : \mathbb{R} \to \{1, 2, \dots, B\}
$$
$$
q(x) =
\begin{cases}
1, & \text{if } -\infty \le x < b_1, \\
2, & \text{if } b_1 \le x < b_2, \\
\vdots & \\
B, & \text{if } b_{B-1} \le x < \infty.
\end{cases}
$$

훈련데이터에대한 의존성을 줄이기 위해 [-15, +15]구간 내에서 **Uniform Binning**을 사용. 값이 15보다 크면 마지막칸, -15보다 작으면 첫번째 칸으로 보냄
시계열 토큰이외에 언어 모델에서 사용하는 특수토큰 PAD, EOS토큰을 어휘에 포함하여 시계열 어휘 집합 $V_{ts}$를 구성
##### 1.2. 언어 모델
 토큰화된 시계열을 일반적인 트랜스포머 기반 언어 모델에 입력. Chronos는 인코더-디코더 모델인 T5계열을 사용, 디코더 only인 GPT-2에도 적용 가능함을 보임
 **모델 아키텍처는 변겨오디지 않으며, 단지 양자화하는 B개수에 따라 어휘 크기 $|V_{ts}|$, 즉 입력층의 크기만 수정하면 됨
 
##### 1.3. 목적 함수
 $$\ell(\theta) = -\sum_{h=1}^{H+1} \sum_{i=1}^{|V_{ts}|} \mathbb{1}(z_{C+h+1}=i) \log p_\theta(z_{C+h+1}=i|z_{1:C+h})$$
 **Cross-Entropy Loss** 그대로 사용함. **다중 분류**문제로 접근하여, 다음 숫자가 **어떤 칸(Bin)**인지 맞추는 문제로 접근**
  손실 함수 자체가 숫자 간의 거리그 자체를 알지 못하지만, 방대한 데이터를 스스로 학습하여 **인접한 칸들은 서로 연관되어 있다**를 학습하는것이 목적임
  
##### 1.4. 예측
 다음에 올 확률이 가장 높은 토큰을 <b>자귀회귀(Autogressive)</b>로 하나씩 뽑아냄
 - **확률론적 예측** : 딱 하나의 값만 내놓는 게 아니라, 여러 번 샘플링하여 미래의 여러 가지 가능성을 그려냄
 - **역과정** : 토큰 샘플링 → 역 양자화(칸 중심값으로 변환) → 역스케일링(원래 단위로 복원) 과정을 거쳐, 우리 눈에 보이는 시계열 데이터로 되돌린다.
 - 
#### 2. 데이터 증강

##### 2.1. TSMixup 📌
 이미지 처리에 사용하는 Mixup의 시계열 버전. 서로 다른 시계열 여러 개를 가져와서 <b>볼록 조합(Convex Combination)</b>을 만듦
 $$\tilde{x}_{1:l}^{\text{TSMixup}} = \sum_{i=1}^k \lambda_i \tilde{x}_{1:l}^{(i)}$$
- e.g. '주가 데이터'와 '온도 데이터'를 특정 비율로 섞어서 <b>세상에 없는 새로운 형태의 시계열</b>을 만들어 모델에게 보여줌으로써, 모델이 특정 데이터에만 매몰되지 않고 강건하게 학습되도록 도움
- 과정 : 무작위로 K개의 시계열(Uniform에서 추출)을 선택 → 평균 스케일링한 후, $Dir({\alpha})$, 디리클레 분포에서 샘플링된 가중치 $\lambda_i$ 를 사용하여 볼록 조합을 생성
 
##### 2.2. KernelSynth
 <b>가우시안 프로세스(GP)</b>를 활용하여 아예 가짜 데이터를 무한정 생산한다.
- <b>커널 뱅크</b> : 선형(추세), RBF(부드러운 변화), 주기성(계절성)등을 담당하는 수학적 커널들을 준비
- <b>조합</b> : 이를 조합하여 복잡한 패턴의 "DNA"를 만듦
- <b>샘플링</b> : 이 DNA로 부터 수학적으로 완벽하게 설명 가능한 시계열을 생성

#### 3. 실험 및 결과

##### 3.1. 평가 지표
- <b>WQL(Weighted Quantile Loss)</b> : 예측값의 분포가 실제 값의 분포를 얼마나 잘 맞췄는지(확률적 정확도)를 측정
- <b>MASE(Mean Absolute Scaled Error)</b> : 점 예측이 얼마나 정확한지를 측정하여, 단순모델 대비 얼마나 나은지를 보여줌

##### 3.2. 하이퍼파라미터 튜닝
- <b>모델 크기</b> : 파라미터가 증가할 수록 성능 향상
- <b>초기화</b> : LM 가중치로 초기화한 모델은 무작위로 초기한 모델에 비해 수렴과 loss가 큼. 따라서 <b>무작위 초기화가 더 나은 선택</b>
- <b>증강</b> : KernelSynth를 통한 합성데이터의 양은, 전체 데이터의 10%정도가 충분
- <b>Context 길이</b> : 1024까지 향상, 그 이후 동일 혹은 약화
- 


---
## 📚 정리

### 📌 제목

#### Chronos: Learning the Language of Time Series

---
---
### 🌟 초록
### 번역

Chronos는 사전 훈련된 확률론적 시계열 모델을 위한 간단하면서도 효과적인 프레임워크를 소개합니다.
Chronos는 <strong>스케일링과 양자화를 사용하여 시계열 값을 고정된 어휘로 토큰화</strong>하고, 이러한 토큰화된 시계열에 대해 <strong>기존의 트랜스포머 기반 언어 모델 아키텍처를 교차 엔트로피 손실(cross-entropy loss)을 통해 훈련</strong>합니다.
저희는 일반화 성능 향상을 위해 <strong>가우시안 프로세스(Gaussian processes)를 통해 생성한 합성 데이터셋</strong>을 보완하여, 대규모 공개 데이터셋 모음에 기반한 T5 계열(20M부터 710M 파라미터까지)의 Chronos 모델을 사전 훈련했습니다.
고전적인 로컬 모델과 딥러닝 방법론을 모두 포함하는 42개 데이터셋으로 구성된 포괄적인 벤치마크에서, 저희는 Chronos 모델이 (a) 훈련 코퍼스에 포함된 데이터셋에서 다른 방법론들을 상당히 능가하며, (b) 해당 데이터셋에 대해 특별히 훈련된 방법론과 비교했을 때 새로운 데이터셋에서 유사하거나 때로는 더 우수한 제로샷(zero-shot) 성능을 보인다는 것을 보여줍니다.
저희 결과는 <strong>Chronos 모델이 다양한 도메인의 시계열 데이터를 활용하여 보지 못한 예측 작업에 대한 제로샷 정확도를 향상</strong>시킬 수 있음을 보여주며, 사전 훈련된 모델을 예측 파이프라인을 크게 단순화할 수 있는 유용한 도구로 자리매김하게 합니다.


---
### 내용

Chornos의 특징
- 스케일랑과 양자화를 사용하여 시계열 값을 고정된 어휘로 <strong>토큰화</strong>하여 <strong>기존의 방식</strong>에 이식
	- 기존의 트랜스포머 기반 모델에 학습
	- 기존의 손실함수인 Cross entropy를 그대로 사용
-  가우시안 프로세스를 통해 <strong>합성 데이터셋을 생성</strong>
이러한 방식으로 zero-shot 성능을 향상 시킴

---
### 포인트

- <strong>기존의 방식</strong>
- <strong>합성 데이터셋</strong>
- <strong>zero-shot</strong>

---
---
### 📌 서론 & 결론 & 고찰

### 번역
#### 서론

시계열 예측은 소매, 에너지, 금융, 의료, 기후 과학 등 다양한 분야에서 의사 결정의 필수적인 구성 요소입니다.
전통적으로 예측은 ARIMA 및 ETS와 같은 통계 모델에 의해 주도되었습니다.
이러한 모델들은 적어도 최근 딥러닝 기술로의 전환(Hyndman & Athanasopoulos, 2018; Benidis et al., 2022)이 있기 전까지는 신뢰할 수 있는 도구 역할을 해왔습니다.
이러한 전환은 대규모의 다양한 시계열 데이터 소스의 가용성과, 딥 예측 모델의 강점, 즉 대규모 시계열 컬렉션에서 패턴을 추출하는 능력에 유리한 운영 예측 문제(Kolassa & Januschowski, 2019)의 출현에 기인할 수 있습니다.
인상적인 성능에도 불구하고, 딥 예측 모델은 여전히 동일한 데이터셋에 대한 훈련 및 예측의 표준적인 방식 내에서 작동합니다.
예측을 위한 전이 학습(transfer learning, Ye & Dai, 2018) 및 도메인 적응(domain adaptation, Jin et al., 2022)에 전념한 연구들이 있었지만, 이 분야는 아직 시계열 연구자들에게 중요한 목표인 통합적이고 범용적인 예측 모델로 수렴되지 못했습니다.

제로샷 학습 기능을 갖춘 대규모 언어 모델(LLM)의 출현은 시계열을 위한 "기초 모델(foundation models)" 개발에 대한 관심을 불러일으켰습니다.
LLM의 맥락에서, 이러한 관심은 두 가지 주요 경로를 통해 추구되었습니다: 자연어를 사용하여 사전 훈련된 LLM을 직접 프롬프트하는 방식(Gruver et al., 2023; Xue & Salim, 2023)과 시계열 작업을 위해 LLM을 미세 조정하는 방식(Zhou et al., 2023a; Jin et al., 2024)입니다.

LLM의 맥락에서, 이러한 관심은 두 가지 주요 경로를 통해 추구되어 왔습니다: 자연어로 사전 학습된 LLM에 직접 프롬프트를 제공하는 것(Gruver et al., 2023; 1 Xue & Salim, 2023)과 시계열 작업을 위해 LLM을 파인튜닝하는 것(Zhou et al., 2023a; Jin et al., 2024)입니다.
그러나 이러한 방법들은 각 새로운 작업에 대한 프롬프트 엔지니어링 또는 파인튜닝의 필요성, 또는 상당한 계산 자원과 추론 시간을 요구하는 대규모 모델(GPT-3 (Brown et al., 2020), Llama 2 (Touvron et al., 2023) 등)에 대한 의존성과 같은 중대한 한계에 직면합니다.
최근 동시 연구(Dooley et al., 2023; Das et al., 2023; Rasul et al., 2023; Woo et al., 2024)에서는 실제 및/또는 합성 시계열 데이터의 대규모 코퍼스에 정교한 시계열 특화 설계를 적용하여 트랜스포머 기반 모델을 사전 학습하는 연구도 진행하고 있습니다.

본 연구에서는 한 걸음 물러서서 다음과 같은 질문을 던집니다: 다음 토큰을 예측하는 언어 모델과 다음 값을 예측하는 시계열 예측 모델 간의 근본적인 차이는 무엇인가요?
유한한 사전에서 오는 토큰과 일반적으로 연속적인 무한한 도메인에서 오는 값이라는 명백한 구분에도 불구하고, 두 노력 모두 근본적으로 미래 패턴을 예측하기 위해 데이터의 순차적 구조를 모델링하는 것을 목표로 합니다.
좋은 언어 모델이라면 시계열에서도 “그냥 작동”해야 하지 않을까요?
이 순진한 질문은 시계열 특화 수정의 필요성에 의문을 제기하도록 우리를 자극했으며, 이에 대한 답을 찾는 과정에서 우리는 시계열 예측을 위해 최소한으로 조정된 언어 모델링 프레임워크인 Chronos를 개발하게 되었습니다.
Chronos는 실제 값을 단순하게 스케일링하고 양자화하여 시계열을 이산적인 빈(bin)으로 토큰화합니다.
이러한 방식으로, 우리는 모델 아키텍처 변경 없이 이 “시계열 언어”에 기성 언어 모델을 훈련시킬 수 있습니다(Chronos의 개략적인 묘사는 그림 1 참조).
놀랍게도, 이 간단한 접근 방식은 효과적이고 효율적임이 입증되었으며, 이는 언어 모델 아키텍처가 최소한의 수정으로 광범위한 시계열 문제를 해결할 수 있는 잠재력을 강조합니다.

유용한 범용 시계열 예측 모델을 개발하는 데 있어서, 공개적으로 사용 가능한 시계열 데이터셋의 부족(양과 질 모두)은 모델링 프레임워크보다 arguably 더 중요합니다.
Chronos를 훈련시키는 데 사용한 포괄적인 공개 데이터셋 모음 외에도, 우리의 접근 방식의 핵심 측면은 TSMixup과 KernelSynth를 포함한 데이터 증강 전략의 통합입니다.
TSMixup은 서로 다른 훈련 데이터셋에서 기본 시계열 세트를 무작위로 샘플링하고, 이들의 볼록 조합을 기반으로 새로운 시계열을 생성합니다. KernelSynth는 가우시안 프로세스를 사용하여 커널 함수를 무작위로 구성하여 합성 시계열을 생성합니다.
이러한 기법들은 시계열 예측에서 작은 훈련 데이터셋의 내재적 한계를 해결하여 모델의 견고성과 일반화 성능을 향상시킵니다.

42개의 데이터셋에 걸친 포괄적인 평가는 Chronos를 인-도메인 및 제로샷 예측 모두에 대한 벤치마크로 확립하며, 전통적인 모델과 작업별 딥러닝 접근 방식 모두를 능가합니다.

주목할 만하게, Chronos는 별도의 작업별 조정 없이 즉시 인상적인 제로샷 예측 성능을 달성합니다.
정확도와 상대적으로 작은 모델 크기는 제로샷 예측 애플리케이션을 위해 더 크고 계산적으로 부담이 큰 모델보다 선호되는 대안으로 자리매김하게 합니다.
고정된 어휘를 통해 작동하는 언어 모델로서의 본질 덕분에, Chronos는 LLM의 미래 발전과 원활하게 통합될 수 있으며, 이는 일반화된 시계열 모델로서 추가 개발에 이상적인 후보가 됩니다.


이 논문의 나머지 부분은 다음과 같이 구성됩니다.
2절에서는 시계열 예측 및 언어 모델에 대한 배경을 소개하고 관련 연구를 논의합니다.
3절에서는 시계열을 위한 제안된 언어 모델링 프레임워크인 Chronos를 설명합니다.
4절에서는 데이터 증강 기법과 합성 시계열 생성 프로세스를 논의합니다.
5절에서는 주요 결과와 다양한 설계 선택에 대한 엄격한 분석을 제시합니다.
6절에서 향후 방향을 논의하고 7절에서 논문을 마무리합니다.
부록에 추가 자료가 제시됩니다.

---
#### 결론

본 연구에서는 미니멀리스트의 관점에서 일반화된 사전 훈련된 예측 모델을 개발하는 문제에 접근합니다.
우리는 기존 언어 모델 아키텍처와 훈련 절차를 시계열 예측에 적용하여, 예측을 위해 시계열 특정 특징이나 아키텍처가 필요하다는 통념에 도전합니다.
이는 역설적으로 시간에 독립적인 시계열을 위한 언어 모델링 프레임워크인 Chronos를 탄생시켰습니다.
Chronos의 특징은 모든 언어 모델 아키텍처와의 호환성이며, 스케일링 및 양자화를 통한 토큰화라는 최소한의 수정만 요구합니다.
우리의 사전 훈련된 모델은 도메인 내 성능 측면에서 기존의 로컬 모델 및 작업별 딥러닝 기준 모델을 상당히 능가합니다.
더욱 주목할 만한 점은, Chronos 모델이 보지 못한 데이터셋(제로샷 성능)에서 우수한 결과를 얻으며, 해당 데이터셋에서 훈련된 최고의 딥러닝 기준 모델과 경쟁력 있는 성능을 보이고, 미세 조정을 통한 추가 개선의 유망한 증거를 보여준다는 것입니다.

우리의 기여는 두 가지 주요 측면에서 중요합니다.
첫째, 기존 언어 모델 아키텍처가 시계열 특정 사용자 정의 없이 예측을 수행할 수 있음을 보여줍니다.
이는 LLM 분야의 발전과 더 나은 데이터 전략을 활용하여 가속화된 발전을 위한 길을 열어줍니다.
둘째, 실질적인 수준에서 Chronos 모델의 강력한 성능은 대규모(예측 기준) 사전 훈련된 언어 모델이 정확도를 희생하지 않고 예측 파이프라인을 크게 단순화할 수 있음을 시사하며, 개별 작업에 대한 모델 훈련 및 튜닝을 포함하는 기존 접근 방식에 대한 추론 전용 대안을 제공합니다.

#### 고찰

Chronos는 실용적인 사전 학습된 시계열 예측 모델의 초기 노력 중 하나로, 포괄적인 테스트 데이터셋 모음에서 주목할 만한 제로샷(zero-shot) 성능을 보여줍니다.
이 연구는 우리가 아래에서 논의할 몇 가지 다양한 연구 방향을 열어줍니다.

6.1 제로샷 단변량 예측을 넘어서

실험에서 우리는 대부분의 데이터셋에 대해 제로샷 방식으로 Chronos를 평가했습니다.
이러한 설정은 제로샷 Chronos 모델이 특정 작업 기반 모델에 비해 경쟁력이 있음을 강조합니다.
우리는 인-도메인(in-domain) 및 제로샷 결과 모두 섹션 5.5.2에서 간략하게 탐색한 파인튜닝(fine-tuning)을 통해 더욱 향상될 수 있을 것으로 예상합니다.
이는 로우랭크 어댑터(low-rank adapters, LoRA) (Hu et al., 2022; Zhang et al., 2023)에 기반한 방법과 같은 모든 파라미터 효율적인 파인튜닝 방법을 사용하여 수행될 수 있습니다.
대안적으로, Chronos는 컨포멀(conformal) 방법 (Romano et al., 2019; Stankeviciute et al., 2021; Xu & Xie, 2021)을 사용하여 특정 작업에 대해 보정될 수 있습니다.
Chronos는 컨포멀 예측(conformal prediction)의 맥락에서 특히 매력적인데, 이는 훈련 세트가 필요하지 않으므로 사용 가능한 모든 데이터를 보정에 사용할 수 있기 때문입니다.

본 연구에서는 가장 일반적인 실제 시계열 사용 사례를 구성하는 균일하게 샘플링된 시계열의 단변량 예측에 초점을 맞추었습니다.
그럼에도 불구하고, 실제 예측 작업은 종종 고려해야 할 외생 정보(exogenous information)를 포함하거나 불규칙하게 샘플링된 시계열의 모델링을 필요로 할 수 있습니다 (Rubanova et al., 2019; Ansari et al., 2023).

그럼에도 불구하고, 실제 예측 작업에는 고려되어야 하는 외생 정보가 포함되거나 불규칙적으로 샘플링된 시계열 모델링을 요구할 수 있습니다 (Rubanova et al., 2019; Ansari et al., 2023).
외생 정보의 한 예시는 시간 독립적(예: 제품의 색상)이거나 시간 가변적(예: 제품이 판매되는 요일)인 공변량입니다.
또 다른 밀접하게 관련된 문제는 다변량 예측으로, 한 시계열(예: 금리)의 과거 값이 다른 시계열(예: 주택 가격)의 예측에 영향을 미칠 수 있습니다.
공변량 또는 다변량 차원의 수는 작업마다 크게 다를 수 있으며, 이는 모든 가능한 조합을 처리할 수 있는 단일 모델을 훈련하기 어렵게 만듭니다.
가능한 해결책은 공변량을 사전 훈련된 예측 모델에 주입하는 작업별 어댑터를 훈련하는 것을 포함할 수 있습니다 (Rahman et al., 2020).
다른 옵션으로는 Chronos와 LightGBM (Ke et al., 2017)과 같이 공변량 처리에 뛰어난 다른 경량 모델의 스태킹 앙상블(Ting & Witten, 1997)을 구축할 수 있습니다.

지금까지 우리의 탐색은 시계열 예측 문제에 집중되었습니다.
그러나 분류, 클러스터링, 이상 탐지와 같은 다른 여러 시계열 분석 작업(Dau et al., 2018; Wu & Keogh, 2021; Ismail Fawaz et al., 2019; Goswami et al., 2024)은 Chronos와 같은 사전 훈련된 모델로부터 잠재적으로 이점을 얻을 수 있습니다.
우리는 Chronos-T5 모델의 인코더에 의해 학습된 표현이 보편적이며 이러한 작업에 사용될 수 있다고 가정합니다.
다양한 다운스트림 작업에 대한 Chronos-T5 표현의 탐색은 흥미로운 향후 연구를 구성할 것입니다.

6.2 추론

더 큰 Chronos 모델의 잠재적인 한계는 작업별 딥러닝 모델에 비해 추론 속도입니다.
그림 17은 데이터셋 전반에 걸쳐 평균화된 단일 시계열에 대한 예측 생성의 추론 시간을 보여줍니다.
더 큰 Chronos 모델의 추론 속도는 일부 통계적 로컬 모델과 유사합니다.
또한, Chronos 모델은 작업별 모델보다 느리지만, 금지될 정도로 느릴 만큼 크지는 않습니다.
더욱이, 작업별 모델은 각 작업에 대해 개별적으로 훈련되어야 하며, 이는 추가적인 시간과 컴퓨팅 자원을 요구합니다.
대조적으로, Chronos 모델은 다양한 이력 길이, 빈도, 예측 지평선 및 컨텍스트 길이를 가진 데이터셋에 배포될 수 있습니다.
이는 모델 배포를 훨씬 더 쉽게 만들고 예측 파이프라인을 대폭 단순화하여 작업별 훈련의 필요성을 없앱니다.

시계열을 위한 언어 모델링 프레임워크를 활용함으로써, 우리는 NLP 커뮤니티의 발전을 Chronos 모델에 즉시 적용할 수 있습니다.
예를 들어, 최신 Ampere GPU에 최적화된 CUDA 커널, 양자화(Dettmers et al., 2022), 그리고 추측적(Leviathan et al., 2023) 및 미리보기(Fu et al., 2023) 디코딩을 포함한 더 빠른 디코딩 기술을 사용하여 추론 속도를 개선할 수 있습니다.
장기 컨텍스트 언어 모델(Sun et al., 2022; Dao, 2023)의 발전은 계절 패턴을 포착하기 위해 더 긴 컨텍스트를 요구하는 고빈도 데이터셋에 대한 Chronos 모델의 적용 가능성을 개선하는 데 도움이 될 수 있습니다.
온도 튜닝, 빔 서치(Freitag & Al-Onaizan, 2017), Top-K 샘플링(Fan et al., 2018), 핵 샘플링(Holtzman et al., 2019)과 같이 텍스트 언어 모델에 널리 사용되는 다른 기법들은 예측 품질을 향상시킬 수 있습니다.
이는 현재 여러 샘플에 대한 집계를 요구하는 포인트 예측의 속도와 품질을 개선하는 데 특히 도움이 될 수 있습니다.

6.3 데이터

우리의 연구 결과는 대규모 시계열 데이터 코퍼스에서 더 큰 모델을 훈련하는 것이 우수한 인-도메인 및 제로샷 성능을 제공한다는 것을 강조합니다.
그럼에도 불구하고, NLP와 대조적으로, 고품질의 공개 시계열 데이터는 여전히 제한적입니다.
이는 다양한 데이터셋의 대규모 코퍼스에서 모델을 훈련할 때 딜레마를 제기합니다. 즉, 훈련을 위해 더 많은 데이터셋을 선택하면 제로샷 평가를 위한 데이터셋이 줄어듭니다.
시계열 커뮤니티는 Chronos와 같은 사전 훈련된 모델을 개발하고 개선하는 데 사용될 수 있는 더 큰 시계열 데이터셋의 가용성으로부터 큰 이점을 얻을 것입니다.
특정 도메인(Emami et al., 2023; Liu et al., 2023) 및 교차 도메인(Borchert et al., 2022)을 위한 대규모 시계열 데이터셋 구축에 대한 최근의 노력들이 있었지만, 추가적인 투자가 필요합니다.

데이터 부족 문제를 해결하기 위한 또 다른 방향은 합성 시계열을 생성하는 더 나은 방법을 개발하는 것입니다.
우리의 연구는 가우시안 프로세스를 사용하여 생성된 합성 데이터의 유용성을 명확하게 입증하고, 훈련 데이터에 통합될 때 모델 성능을 개선함으로써 이 방향으로 상당한 발전을 이루었습니다.
합성 데이터만으로 훈련된 모델조차도 합리적인 예측 성능을 보입니다.
향후 연구에서는 이러한 모델의 실패 모드를 심층적으로 분석하고, 실제 데이터와 합성 데이터 간의 격차를 해소하기 위한 개선 방안을 제안할 수 있습니다.



---
---
### 내용
#### 서론

ARIMA, ETS → DL 전환전까지 지배적이었음
DL로 전환이후 인상적인 성능 향상이 었더라도, 동일한 데이터셋 대한 훈련 및 예측이 표준적인 방식안에서 작동함
Transfer Learning, Domatin Adaptation과 같은 연구가 있었지만 시계열의 중요한 목표인 통합적이고 범용적인 모델의 설계에는 한계점이 있음

LLM의 등장으로 zero-shot에 대응가능한 foundation 모델에 대해서 관심이 증가하였다.
- 자연어를 사용하여 LLM을 직접 프롬프팅하는 방식
- 시계열 작업을 위해 LLM을 fine tunning하는 방식
→ 이는 <strong>프롬프트 엔지니어링, 파인튜닝의 필요성 그리고 많은 계산자원이 필요한 GPT-3나 llama2등에 대해 의존성이 존재</strong>한다.

최근에는 실제 혹은 합성 시계열을  통해서 시계열에 특화된 트랜스포머 설계를 진행하는 연구도 진행되고 있음

이 논문은 <strong>다음 토큰을 예측하는 자연어 모델 vs 다음 값을 예측하는 시계열 모델의 근본적인 차이</strong>점에 대해서 고찰함
- 유한한 코퍼스 vs 무한한 도메인에도 불구하고 → <strong>미래 패턴을 예측하기 위해 데이터를 순차적으로 모델링하는것을 목표로 하는것은 같음 </strong>
<strong>따라서 과연 시계열에 특화된 수정이나 설계가 필요있을까에 대한 근본적인 질문을 하게 함</strong>

Chronos는 시계열 예측을 위해 최소한으로 LLM을 수정한 모델이며, 시계열 예측을 위해 실제값을 단순하게 스케일링 및 양자화를 통해 시계열을 이산적인 bin으로 토큰화 한다.
이러한 접근방식은 <strong>시계열을 "시계열 언어"로서 언어 모델에 훈련</strong>시킬 수 있도록 한다. 이러한 방식은 놀랍도록 효과적임

따라서 모델보다는 기법들이 중요하며, 데이터 부족을 해결하기 위해 아래의 증강을 적용하였다.
- <strong>TSMixup</strong> : 서로 다른 훈려셋에서 기본 시계열 세트를 무작위로 샘플링하고, 볼록조합을 통한 새로운 시계열을 생성하는 방식
- <strong>KernelSynth</strong> : 가우시안 프로세스를 통해 커널 함수를 무작위로 구성하여 합성 시계열을 생성
→ 이러한 접근으로 부족한 시계열 데이터에 대해 일반화와 모델의 견고성 둘 다 챙길 수 있도록 하였다.

별도의 파인튜닝없이 상당한 zero-shot성능을 가지며, 모델이 가볍기 때문에 효과적이다. 추가적으로 고정된 어휘를 사용하는 언어 모델로서 접근하였기 때문에, LLM에 통합도 가능하다.

![](/assets/img/posts/chronos/fa5c3bb8a17b181478e563e574662a0c.png)
> Figure 1


---
#### 결론

시계열 특화 모델을 만드는것에 반대로 접근하며, <strong>모든 언어모델에 대해 호환가능</strong>하며 이는 최소한의 수정을 통해 이루어진다.
- 스케일링과 양자화 → 토큰화
파인튜닝을 하지 않고도 인상적인 zero-shot성능을 보이며, 파인튜닝을 진행하면 얼마나 좋을지에 대해서 기대할 수 있음

Chronos는
1. 기존 언어 모델 아키텍처를 시계열에 적용할 수 있음을 보여줌
2. 예측 파이프라인을 단순화 시킬 수 있음(모델을 그대로 사용하면 되기에) → 추론에 대해 새로운 관점으로 접근할 수 있도록 함 

---
---
### 포인트
#### 서론

> [!NOTE]
> <strong>ARIMA</strong>
> 시계열을 `자기회귀(AR) + 이동평균(MA) + 차분(I: differencing)`으로 설명하는 고전적인 통계모델
> 보통 $ARIMA(p, d, q)$로 표기하며, d번 차분으로 정상성을 확보한뒤 자기상관을 모델링함(ACF/PACF)
> - 강점 : 데이터수가 작아도 잘 작동, 해석 가능성(자기상관), 예측구간을 이론적으로 계산 가능
> - 한계 : 강한 비선형 & 복잡한 패턴에는 약함, 계절성 및 구조변화가 있으면(잔차가 비정규 혹은 자기상관이 남으면) 성능이 흔들릴 수 있음
> 1. <strong> 자기회귀</strong>(AR, p : 현재값을 설명하기 위해 과거의 데이터를 몇개까지 볼 것인지) : <strong>과거의 자기자신이 현재의의 자기자신을 결정</strong>
> 2. <strong>차분</strong>(I, d : 몇번이나 뺄건지) : <strong>"정상성"을 맞추기 위해 현재 값에서 과거 값을 빼는 것.</strong> 이는 데이터가 우상향이거나 계절성이 있으면 통계적 분석이 어렵기 때문에, 차분을 통해 데이터의 평균및 분산을 일정하게 만드는것
> 3. <strong>이동평균</strong>(MA, q : 과거의 오차를 몇 개까지 반영할것인가) : <strong>과거의 예측오차가 현재를 결정하는 것</strong>
> 4. <strong>ACF</strong>(자기상관함수) : 시차에 따른 $y_t$와 $y_{t-k}$사이의 상관관계 측정, MA(q)의 차수를 구하기 위해 사용. ACF그래프가 특정 시점이후에 0으로 갑자기 떨어진다면, 그 지점이 q임
> 5. <strong>PACF</strong>(부분자기상관함수) : 두 점 사이의 상관관계를 측정하되, 그 사이의 지점들의 영향력을 제거한 순수한 상관관계만 봄. AR(p)의 차수를 결정할 때 사용. PACF그래프가 특정 시점이후에 0으로 갑자기 떨어진다면, 그 지점이 p임

> [!NOTE]
> <strong>ETS(Error-Trend-Seasonal)</strong>
> 지수평활(exponential smoothing)은 최근 관측치에 더 큰 가중치를 두는 기법이고, 이를 <strong>상태공간</strong>에 정식화하여 확률모형과 AIC(자동 모형 선택)등에 사용가능하도록 만든 체계
> - 강점 : 추세와 계절성이 명확한 <strong>비정상 시계열</strong>에 강하며, 실무 업무 예측에 널리 사용
> - 한계 : 세상은 기본적으로 비정상적이라는 철학으로 설계함, 따라서 정상성이 필요한 데이터에는 <strong>ARIMA</strong>가 더 자연스러울 수 있음
> 1. <strong>정상성(Stationarity)</strong> :  <strong>데이터가 시간이 지나도 변화하지 않는 성질</strong>
> 	- 평균이 일정함
> 	- 분산이 일정함
> 	- 공분산이 일정함
> 2. 정상성을 이루기 위한 차분
> 	- 1차 차분 : $y_t - y_{t-1}$을 사용하여 추세를 제거
> 	- 로그 변환 : 변동 폭이 가면갈수록 커지는 경우
> 	- 계절 차분 : $y_t - y_{t-m}$을 사용하여 계절성을 제거


> [!NOTE]
> <strong>LLM Zero-Shot Forecasters (Gruver et al., 2023)</strong>
> <strong>시계열 값을 숫자 문자열로 인코딩하여 "다음 토큰 예측"문제로 바꿔버림</strong>. 이를 통해 GPT-3, LLaMA-2와 같은 모델에 추가학습없이 외삽을 꽤나 잘한다 → Zero-shot가능성
> - 숫자 토크나이징/디코딩을 설계 → 토큰 분포의 연속값을 확률분포로 바꾸는 절차를 제안
> - GPT-4가 숫자 토크나이징&정렬(RLHF) 영향으로 GPT-3보다 성능이 떨어질 수 있음
> - 시사점 : 텍스트 사전학습 모델이 시계열에도 적용이 가능함을 시사
> - 한계점 : 숫자 표현 및 토크나이저 설계에 민감하며, 스케일에 따라 성능차이가 큼
> 1. 외삽 : 과거 데이터 패턴을 보고, 아직 오지 않은 미래의 값을 예측

> [!NOTE]
> <strong>Time-LLM: Time Series Forecasting by Reprogramming LLMs (Jin et al., ICLR 2024)</strong>
> <strong>LLM을 frozen시키고 입력시계열을 텍스트로 재프로그래밍(Reprogramming)하여 LLM이 다루기 쉬운 형태롤 정렬시키는 프레임워크</strong>
> “Prompt-as-Prefix(PaP)”로 문맥을 풍부하게 주고, LLM 출력(변환된 패치)을 다시 예측 값으로 사상(projection)
> 1. 재프로그래밍 : 언어적 형태로 바꾸어줌
> 2. PaP(Prompt-as-Prefix)와 사상(Projection) : PaP는 "텍스트 설명"을 접두사로 달아주는 것(이 데이터는 어떤 데이터이고, 어떤 패턴을 가지고 있다), 사상은 실수값으로 변환하는 것

> [!NOTE]
> <strong>ForecastPFN (Dooleyet al.2023):</strong>
<strong>합성(synthetic) 데이터 분포</strong>를 활용하여 훈련된 최초의 zero-shot 예측 모델. 새 시계열 데이터에 대해 <strong>베이지안 근추론을 근사</strong>하도록 학습시키는 것
> 새로운 데이터에 대해 <strong>재학습 없이 단 한번의 순전파로 예측</strong>
적은 데이터로도 기존 최첨단(SOTA) 모델보다 정확하고 빠른 예측 성능 입증.
> - 핵심 주장 : 작은 데이터포인트를 가지고, 기존보다 빠르고 정확하게 예측(논문 주장 40개 이하)
> - 한계점 : 어떤 합성 데이터로 학습했는가가 실제 도메인간 불일치가 발생한 경우 리스크가 발생함
> 1. PFN :  <strong>실제 데이터가 없으면, 가짜 데이터를 수조개 만들어서 학습시키자</strong>로 다음과 같은 작동방식을 가짐
> 	1. 통계적 함수(ARIMA, ETS등)들을 활용하여, 무작위로 조합해 <strong>가상의 시계열</strong>을 생성
> 	2. 이 가상의 데이터로 정답을 맞추도록 학습
> 	3. 가짜 패턴들 중 비슷한 패턴을 찾아 실제 데이터와 비슷하다고 판단하여 예측

> [!NOTE]
> <strong>A decoder-only foundation model for time-series forecasting (Das et al., 2023/ICML 2024)</strong>
대규모 시계열 코퍼스(corpus)에서 사전 학습된 decoder-only Transformer 기반 모델.
다양한 public 데이터셋에서 zero-shot 성능이 SOTA 지도 학습(supervised) 모델에 근접.
다양한 예측 이력 길이, 예측 길이, 시간적 세분성에 잘 작동.


> [!NOTE]
> <strong>Moirai (Woo et al., 2024) — Unified Training of Universal TS Forecasting Transformers</strong>
Masked Encoder 기반의 Universal Time Series Forecasting Transformer.
시계열 데이터의 고유한 도전 과제(교차 주파수 학습, 다양한 변수 수, 분포 특성)를 해결하기 위해 Transformer 아키텍처를 개선.
270억 개 이상의 관측치를 포함하는 대규모 공개 시계열 아카이브(LOTSA)로 훈련.
zero-shot 예측에서 full-shot 모델과 비교하여 경쟁력 있거나 우수한 성능 달성.

> [!NOTE]
> <strong>BIN (Binning in Time-Series)</strong>
> 시계열 윈도우 내의 연속적인 수치를 이산적인 구간(Bin)으로 나누어 토큰화 하는 기법
> - 강점 : 수치 데이터의 스케일 문제를 완화, LLM에 그대로 사용 가능
> - 단점 : 구간 개수나, 경계 설정 전략에 따라 정보 손실 가능


---
#### 결론

(없음)

---
---
## 🔬 핵심

### 📚 3.1. 시계열 토큰화
### 번역

예측 구간이 $H$인 시계열 $x_{1:C+H} = [x_1, \dots, x_{C+H}]$를 고려해 봅시다. 여기서 처음 $C$개의 시점은 과거 맥락(historical context)을 구성하고, 나머지 $H$개는 예측 구간(forecast horizon)을 나타냅니다.
언어 모델은 유한한 어휘 집합의 토큰으로 작동하므로, 이를 시계열 데이터에 사용하려면 관측값 $x_i \in \mathbb{R}$을 유한한 토큰 집합으로 매핑해야 합니다.
이를 위해 먼저 관측값을 스케일링한 후 고정된 수의 구간(bin)으로 양자화합니다.

##### 스케일링

시계열의 스케일은 단일 데이터셋 내에서도 상당히 다를 수 있습니다. 이는 딥러닝 모델의 최적화에 어려움을 야기합니다. 따라서 개별 시계열은 더 나은 최적화를 위해 정규화됩니다. Chronos의 경우, 정규화의 목표는 시계열 값을 양자화에 적합한 범위로 매핑하는 것입니다.

일반적인 정규화 기법은 시계열에 아핀 변환(affine transformation)을 적용하는 것을 포함합니다. 즉, $\tilde{x}_i = (x_i - m)/s$입니다.
평균 스케일링(mean scaling), 표준 스케일링(standard scaling), 최소-최대 스케일링(min-max scaling)과 같은 여러 인기 있는 정규화 방식은 $m$과 $s$를 적절히 선택함으로써 얻을 수 있습니다.

저희는 평균 스케일링을 선택했는데, 이는 실제 시계열 응용에 흔히 사용되는 딥러닝 모델에서 효과적인 것으로 입증된 방법입니다(Salinas et al., 2020; Rabanser et al., 2020). 하지만 다른 접근 방식도 가능하며 최소한의 변경만 필요합니다.
평균 스케일링의 매력적인 특징은 시계열의 $0$ 값을 보존한다는 것입니다. 이러한 $0$ 값은 종종 의미론적으로 중요하며, 예를 들어 제품의 판매량 $0$ 또는 밤의 태양 에너지 발전량 $0$과 같습니다.

평균 스케일링은 과거 맥락(historical context) 내 절대값들의 평균으로 개별 시계열 항목을 정규화합니다. 구체적으로, 이는 $m = 0$이고 $s = \frac{1}{C} \sum_{i=1}^C |x_i|$로 설정하는 것을 포함합니다.

##### 양자화

스케일링된 시계열 $\tilde{x}_{1:C+H} = [\tilde{x}_1, \dots, \tilde{x}*C, \dots, \tilde{x}*{C+H}]$는 여전히 실수 값을 가지며 언어 모델에서 직접 처리할 수 없습니다. 이러한 실수 값을 이산적인 토큰으로 변환하기 위해 양자화를 사용합니다.

형식적으로, 실수선 상에서 $B$개의 구간 중심점 $c_1 < \dots < c_B$와 이들을 구분하는 $B-1$개의 경계 $b_i$ $(c_i < b_i < c_{i+1})$를 선택합니다. 여기서 $i \in {1, \dots, B-1}$입니다.
그러면 양자화 함수 $q: \mathbb{R} \to {1, 2, \dots, B}$와 역양자화 함수 $d: {1, 2, \dots, B} \to \mathbb{R}$는 다음과 같이 정의됩니다.

$$
q(x) = \begin{cases}
1 & \text{if } -\infty \le x < b_1, \
2 & \text{if } b_1 \le x < b_2, \
\dots \
B & \text{if } b_{B-1} \le x < \infty,
\end{cases}
\quad \text{and} \quad
d(j) = c_j,
$$

각각에 대해.
구간 중심점과 경계의 위치는 데이터 종속적(data-dependent)이거나 균일(uniform)할 수 있습니다(Rabanser et al., 2020). 데이터 종속적 구간화의 한 종류인 분위수 구간화(quantile binning)는 훈련 데이터포인트의 누적 분포 함수(CDF)를 활용하여 각 구간에 대략적으로 동일한 수의 데이터포인트가 할당되도록 구간을 구성합니다.

반면, 균일 구간화는 구간 $[c_1, c_B]$ 내에서 균일하게 간격이 떨어진 구간 중심점을 선택하며, 구간 경계는 연속된 구간 중심점들 사이의 중간 지점에 위치합니다. 즉, $b_i = \frac{c_i + c_{i+1}}{2}$ for $i \in {1, \dots, B-1}$입니다.

보지 못한 다운스트림 데이터셋의 값 분포는 훈련 분포와 상당히 다를 수 있으므로, 저희는 실험에서 균일 구간화를 선택했지만 다른 양자화 기법도 사용될 수 있습니다. 시계열 양자화 기법에 대한 자세한 논의는 Rabanser et al. (2020)을 참조하십시오.
이 접근 방식의 잠재적인 한계는 예측 범위가 $[c_1, c_B]$로 제한되어, 강한 추세를 가진 시계열을 모델링하는 것이 이론적으로 불가능하다는 것입니다. 이 내용은 섹션 5.7에서 실제적인 맥락에서 더 자세히 탐구합니다.

시계열 토큰 ${1, 2, \dots, B}$ 외에도, 언어 모델에서 흔히 사용되는 두 개의 특수 토큰인 PAD와 EOS를 시계열 어휘 집합 $V_{ts}$에 포함합니다.
PAD 토큰은 배치 구성을 위해 길이가 다른 시계열을 고정된 길이로 패딩(padding)하거나 누락된 값을 대체하는 데 사용됩니다.

EOS 토큰은 양자화되고 패딩된 시계열에 추가되어 시퀀스의 끝을 나타냅니다.
시계열의 경우 EOS 토큰 사용이 엄격하게 필수적이지는 않지만, 인기 있는 언어 모델링 라이브러리를 사용한 학습 및 추론을 편리하게 만듭니다.
$V_{ts}$의 토큰 시퀀스는 인코더-디코더 및 디코더 전용 모델 모두에서 언어 모델에 의해 쉽게 처리될 수 있어, 일반적인 방식으로 학습할 수 있습니다.
시계열 모델링에서 일반적인 접근 방식은 요일, 연중 주차 등과 같은 특징을 통해 시간 및 주파수 정보를 통합하는 것입니다.
역설적으로 들릴 수 있지만, Chronos에서는 시간 및 주파수 정보를 무시하고 "시계열"을 단순히 시퀀스로 취급합니다.

우리는 주로 인코더-디코더 T5 모델(Raffel et al., 2020)의 변형에 초점을 맞춥니다.
추가적으로, GPT-2(Radford et al., 2019) 모델을 사용한 실험을 수행하여 우리의 접근 방식이 디코더 전용 모델로 쉽게 확장될 수 있음을 보여줍니다.
언어 모델 아키텍처에는 수정이 필요하지 않으며, 양자화에 사용된 빈의 수에 따라 달라지고 원래 언어 모델의 어휘 크기와 다를 수 있는 $|V_{ts}|$로 어휘 크기를 조정하는 것만 제외하면 됩니다.
구체적으로, 어휘 크기를 조정하는 것은 언어 모델의 입력 및 출력 임베딩 레이어를 잘라내거나(또는 확장하는 것)을 포함합니다.

---
### 내용

예측 구간이 $H$인 시계열 $x_{1:C+H} = [x_1, \dots, x_{C+H}]$에서 처음 C개의 시점은 과거 맥락을 구성하고, 나머지 $H$개는 에측 구간을 나타냄
언어 모델은 <strong>유한한 어휘 집합</strong>의 토큰으로 작동하므로, 이를 시계열에 적용하기 위해서는 시계열 관측값을 $x_i \in \mathbb{R}$을 <strong>유한한 토큰</strong> 집합으로 매핑하여야 함

##### 스케일링

<strong>시계열의 스케일은 단일 데이터셋 내에서도 다를 수 있다.</strong> 따라서 개별 시계열을 모델 최적화를 위해 정규화를 진행하며, Chronos의 경우 정규화의 목표는 양자화에 적합한 범위로 매핑하는 것
시계열 정규화 기법
- <strong>아핀 변환(affine transformation)</strong>을 적용하여, $\tilde{x}_i = (x_i - m)/s$ 를 만든다.
- 이는 mean, standard, min-max sacling등 여러 인기있는 정규화 방식을 m과 s를 얻을 수 있다.
Chronos에서는<strong> mean saciling을 택했고</strong> 이는 딥러닝 모델에서 효과적인것으로 입증된 "(Salinas et al., 2020; Rabanser et al., 2020)"논문을 근거로 함, 하지만 다른 방법으로 접근 가능하며 최소한의 수정만 거치면 됨
평균 스케일링의 매력적인 부분은 0값을 보존하여 의미론적으로 서로 다른 도메인의 0끼리 일치시키는 역할을 함
e.g. 제품 판매량 0 = 밤의 태양 에너지량 0

 $m = 0$이고 $s = \frac{1}{C} \sum_{i=1}^C |x_i|$ 이 나온 계기
 - 보통 m에 평균을 놓으면, 중심이 0으로 이동하게 됨, 그러나 Chronos에서는 <strong>데이터의 절대적인 크기나 '부호'정보를</strong> 유지하기 위하여 m=0으로 둠
 - 과거 문맥 C개의 데이터포인트 절대값의 평균으로 값이 아주큰걸 효과적으로 줄이고, 값이 아주 작은걸 효과적으로 늘리는 역할
결론적으로 <strong>데이터의 중심축을 건드리지 않은채, 전체적인 덩치를 조정하는</strong>방식

##### 양자화

스케일링된 시계열은 여전히 $\tilde{x}_{1:C+H} = [\tilde{x}_1, \dots, \tilde{x}_{C}, \dots, \tilde{x}_{C+H}]$ 실수 값을 가져서 언어 모델에 넣을 수 없음.

따라서, B개의 구간을 나누는 B개의 구간 중심점 $c_1 < \dots < c_B$, 이를 구분하는 B-1개의 경계 $b_i$ $(c_i < b_i < c_{i+1})$를 선택한다. 이때 i 는 1~(B-1)까지이다.
그렇게 되면 양자화함수와 역양자화 함수는 다음과 같이 정의 된다.(q:양자화, d:역양자화)
$q: \mathbb{R} \to {1, 2, \dots, B}$, $d: {1, 2, \dots, B} \to \mathbb{R}$
$$
q(x) = \begin{cases}
1 & \text{if } -\infty \le x < b_1, \
2 & \text{if } b_1 \le x < b_2, \
\dots \
B & \text{if } b_{B-1} \le x < \infty,
\end{cases}
\quad \text{and} \quad
d(j) = c_j,
$$
각각에 대해 구간의 중심점 $c_i$와 경계의 위치 $b_i$는 <strong>데이터 종속적이거나 균일</strong>할 수 있다.
- <strong>분위수 구간화</strong> : 데이터 종속적 구간화 중 한 종류로서, 훈련 데이터의 데이터포인트의 CDF를 활용하여, 각 구간에 대략적으로 동일한 수의 데이터 포인트가 할당되도록 함.
- <strong>균일 구간화</strong> : $[c_1, c_B]$ 에서 균일하게 간격이 떨어진 구간 중심점 $c_i$를 선택하며, 각 구간 경계는 구간 중심점의 중간 지점에 위치 $b_i = \frac{c_i + c_{i+1}}{2}$ for $i \in {1, \dots, B-1}$
Chronos에서는 <strong>균일 구간화</strong>를 선택하여, 보지 못한 다운스트림 데이터셋에서 훈련 분포와 다를 수 있는점을 고려하였다. 이 방식의 한계는 시계열의 예측범위를 $[c_1, c_B]$내로 제한시켜, 강한 추세를 가진 시계열을 모델링 하는것의 이론적으로 어렵다는것을 시사한다.

시계열 토큰 $[1:B]$ 이외에도, 언어 모델에서 사용하는 특수 토큰 `PAD, EOS`를 추가하였음
- <strong>PAD</strong> : 길이가 다른 시계열을 고정된 길이로 패딩하거나, 누락된 값을 대체
- <strong>EOS</strong> : 시퀀스의 끝을 나타냄, 시계열의 경우 EOS 토큰 사용에 대해 엄격하지 않지만, 언어 모델을 사용할때 학습 및 추론을 쉽게 하기 때문에 추가함. 
이 두 토큰을 시계열 어휘집합 $V_{ts}$에 포함하여 <strong>일반적인 방식으로 학습이 가능</strong>함
시계열을 모델링 하는것은 요일, 주차 등등 특징을 파악해 시간 및 주파수 정보를 통합하는 것이 일반적인데, <strong>Chronos에서는 시간 및 주파수 정보를 무시하고 "시계열"을 단순 시퀀스로 취급한다.</strong>
Chronos는 인코더-디코더 T5 모델(Raffel et al., 2020)의 변형에 초점을 맞추며, 다른 모델로 변형할때에는 $|V_{ts}|$의 어휘 크기만 조정하면 된다.(어휘 크기를 조정하는건 언어 모델을 수정하는것또한 포함한다. 레이어수 등등)
추가적으로, GPT-2(Radford et al., 2019) 모델을 사용한 실험을 수행하여 해당 접근 방식이 디코더 전용 모델로 쉽게 확장될 수 있다.

---
### 포인트

> [!NOTE]
> <strong>아핀 변환(affine transformation)</strong>
> $$\tilde{x}_i = (x_i - m)/s$$
> <strong>평행이동, 선형변환</strong>을 결합한것으로 쉽게 말해, 그래프를 밀거나 당기고(더하기, 뺴기) / 늘리고 줄이는(곱하고, 나누고) 모든 행위를 의미한다
> 

---

### 📚 4. Data Augmentation
### 번역

4 데이터 증강

공개 시계열 데이터의 품질과 양은 WikiText-103 (Merity et al., 2016), C4 (Raffel et al., 2020), The Pile (Gao et al., 2020)과 같은 풍부하고 고품질의 텍스트 데이터셋을 활용하는 자연어 처리(NLP) 도메인과 비교할 때 미미합니다.
이는 다양한 패턴을 가진 대규모 시계열 데이터에 의존하는 제로샷 예측(zero-shot forecasting)을 위한 모델 훈련에 어려움을 야기합니다.
이 문제를 해결하기 위해, 실제 데이터셋에서 믹스업 증강(mixup augmentations)을 생성하고 합성 데이터로 훈련을 보완함으로써 훈련 데이터의 다양성을 향상시킬 것을 제안합니다.

## 4.1 TSMixup: 시계열 믹스업

믹스업(Mixup, Zhang et al., 2017)은 이미지 분류 맥락에서 제안된 데이터 증강 기법입니다.
이는 훈련 데이터셋에서 무작위 이미지 쌍과 해당 레이블의 볼록 조합(convex combinations)을 생성하여, 딥러닝 모델의 암기(memorization) 및 과적합(overfitting)과 같은 문제를 완화합니다.
기존 연구들(Carmona et al., 2021; Zhou et al., 2023b)은 믹스업을 시계열 도메인으로 확장했습니다.

이러한 연구들을 바탕으로, 우리는 믹스업의 아이디어를 두 개 이상의 데이터포인트로 일반화하는 TSMixup을 제안합니다.
구체적으로, TSMixup은 훈련 데이터셋에서 특정 길이 $l \sim U{l_{\min}, l_{\max}}$의 시계열 $k \sim U{1, K}$개를 무작위로 샘플링하고, 이를 스케일링한 후 볼록 조합을 취합니다.

$$
\tilde{x}*{TSMixup}^{1:l} = \sum*{i=1}^{k} \lambda_i \tilde{x}^{(i)}_{1:l}
$$

여기서 $\tilde{x}^{(i)}_{1:l}$는 $i$번째 스케일링된 시계열을 나타냅니다.
시계열은 혼합 전에 스케일링되어, 작고 큰 값을 가진 시계열이 혼합 과정에서 동등한 중요도를 갖도록 합니다.
결합 가중치 $[\lambda_1, \dots, \lambda_k]$는 스칼라 농도 파라미터 $\alpha$로 매개변수화된 대칭 디리클레 분포 $\mathrm{Dir}(\alpha)$에서 샘플링됩니다.
TSMixup의 완전한 의사 코드는 부록 A의 알고리즘 1에서 찾을 수 있습니다. 직관적으로, TSMixup은 서로 다른 시계열의 패턴을 결합하여 데이터의 다양성을 향상시킵니다.
그림 2는 TSMixup으로 생성된 증강의 예시를 보여주며

다양한 패턴이 어떻게 혼합되는지를 보여줍니다.

## 4.2 KernelSynth: 가우시안 프로세스를 이용한 합성 데이터 생성

TSMixup이 패턴 다양성을 향상시키지만, 특히 실제 데이터가 제한적일 때 일반적인 시계열 모델을 훈련하기에는 여전히 불충분할 수 있습니다.
훈련 데이터셋을 추가로 보완하기 위해, 우리는 가우시안 프로세스(GP)를 사용하여 합성 시계열을 생성하는 방법인 KernelSynth를 제안합니다.
KernelSynth는 Automatic Statistician (Duvenaud et al., 2013)에서 영감을 받았으며, 여기서 GP 커널 공간에 대한 합성 탐색을 수행하여 시계열의 구조를 설명합니다.
우리는 이 과정의 역을 사용하여 — GP 커널을 무작위로 합성하여 새로운 시계열을 생성합니다.

GP는 평균 함수 $m(t)$와 양의 정부호 커널 $\kappa(t, t')$에 의해 정의되는 함수에 대한 분포이며, 여기서 $t \in \mathbb{R}$는 도메인입니다.
커널은 입력 도메인의 임의의 두 점 $(t, t')$에서의 함수 값의 결합 변동성을 정의하는 공분산 함수를 지정합니다.
커널을 적절하게 선택함으로써 다양한 패턴을 생성할 수 있습니다.
우리는 기본적인 시계열 패턴을 정의하는 기저 커널들의 커널 뱅크 $K$를 구축했습니다.
여기에는 추세를 위한 선형 커널, 부드러운 국소 변동을 위한 RBF 커널, 일반적인 시계열 주파수에서 발견되는 계절성을 위한 주기적 커널이 포함됩니다.
최종 커널 $\tilde{\kappa}(t, t')$은 $K$에서 $j \sim U{1, J}$개의 커널을 복원 추출(with replacement)로 샘플링하고 이 커널들을 무작위 이항 연산인 $+$ 또는 $\times$를 통해 결합하여 구성됩니다. 합성 시계열은 GP 사전 분포 $GP(m(t) = 0, \tilde{\kappa}(t, t'))$에서 길이 $l_{syn}$의 샘플을 추출하여 생성됩니다. 자세한 내용은 부록 A의 알고리즘 2를 참조하십시오.

최종 커널인 $\tilde{\kappa}(t, t')$은 $K$에서 $j \sim U{1, J}$ 커널을 복원 추출하여 샘플링하고, 이 커널들을 무작위 이진 연산인 $+$ 또는 $\times$를 통해 결합함으로써 구성됩니다. 합성 시계열은 GP 사전 분포인 $GP(m(t) = 0, \tilde{\kappa}(t, t'))$로부터 길이 $l_{syn}$의 샘플을 추출하여 생성됩니다. 자세한 내용은 부록 A의 알고리즘 2를 참조하십시오. 그림 3은 KernelSynth에서 사용된 이 생성 과정을 보여주며, 간단한 기저 커널의 조합으로부터 어떻게 복잡한 패턴을 가진 시계열이 발생할 수 있는지 설명합니다.


---
### 내용

시계열 데이터가 자연어 모델에 비해서 턱없이 부족하기 때문에, <strong>합성 시계열 데이터</strong>를 활용하여 증강을 진행한다.

##### TSMixup

이미지 증강 기법인 Mixup을 시계열 도메인으로 확장한 방법
- <strong>Mixup</strong> : 훈련 데이터셋에서 무작위 이미지 쌍과 해당 레이블의 볼록 조합(convex combinations)을 생성하여, 딥러닝 모델의 memorization과 과적합을 완화
이를 두 개 이상의 데이터포인트로 일반화 하는 TSMixup으로 수정하였음(제안)
TSMixup은 훈련 데이터셋에서 특정 길이 $l \sim U{[l_{\min}, l_{\max}]}$의 시계열 $k \sim U[{1, K}]$개를 무작위로 샘플링하고, 이를 스케일링한 후 볼록 조합을 취한다.

$$
\tilde{x}^{TSMixup}_{1:l} = \sum_{i=1}^{k} \lambda_i \tilde{x}^{(i)}_{1:l}
$$

여기서 $\tilde{x}^{(i)}_{1:l}$는 $i$번째 스케일링된 시계열을 나타낸다.
스케일링을 통해서 작거나 큰 값 모두가 동등한 중요도르 가지게 되며, 결합 가중치 $[\lambda_1, \dots, \lambda_k]$는 디리클레 $\mathrm{Dir}(\alpha)$에서 파라미터 $\alpha$로 샘플링 됨
서로 다른 시계열을 합성하여 다양한 시계열을 보여주게 됨

![](/assets/img/posts/chronos/f5f64cb97a72a102cc055bc67e8b60ee.png)

##### KernelSynth

TSMixup이 패턴의 다양성을 향상시킨다면, KernelSynth는 부족한 데이터수를 <strong>가우시안 프로세스를 통해 합성 시계열을 생성</strong>하는 방식이다.
Automatic Statistician (Duvenaud et al., 2013)에서 영감을 받았다. 이 논문에서 GP 커널 공간에 대한 합성 탐색을 수행하여 시계열의 구조를 설명하는데, 이 역 과정을 사용하여 GP커널을 무작위로 합성하여 새로운 시계열을 생성하는 방식이다.
<strong>GP</strong>
- 평균 함수 $m(t)$와 양의 정부호 커널 $\kappa(t, t')$에 의해 정의되는 함수에 대한 분포 이때 $t \in \mathbb{R}$는 도메인
커널은 입력 도메인의 임의의 두 점 $(t, t')$에서 함수 값의 결합 변동성을 정의하는 공분산 함수를 지정한다.
이를 적절하게 선택하믕로서 다양한 패턴을 생성할 수 있다.
기본적인 시계열 패턴을 정의하는 기저 커널들의 커널 뱅크 $K$를 구축하였고, 여기에는 추세를 위한 선형 커널, 부드러운 국소 변동을 위한 RBF 커널, 일반적인 시계열 주파수에서 발견되는 계절성을 위한 주기성 커널이 포함된다.
최종 커널 $\tilde{\kappa}(t, t')$은 $K$에서 $j \sim U[{1, J}]$개의 커널을 복원 추출(with replacement)로 샘플링하고 이 커널들을 무작위 이항 연산 $+$ 또는 $\times$를 통해 결합한다.
합성 시계열은 GP 사전 분포 $GP(m(t) = 0, \tilde{\kappa}(t, t'))$에서 길이 $l_{syn}$의 샘플을 추출하여 생성된다.

![](/assets/img/posts/chronos/c2c8722643976e9d8130b391e37aaaef.png)

---
### 포인트

> [!NOTE]
> <strong>Mixup</strong>
> <strong>볼록 조합 = 가중 평균</strong>과 동일하다.
> $$\tilde{x} = \lambda x_i + (1 - \lambda) x_j$$$$\tilde{y} = \lambda y_i + (1 - \lambda) y_j$$
> e.g. 예를 들어, 개와 고양이를 0.7개의 개, 0.3개의 고양이로 만드는것과 동일하다.
> 

> [!NOTE]
> <strong>TSMixup</strong>
> 동작 원리
> 1. 데이터 쌍 선택: 훈련셋에서 무작위로 두 개의 시계열 샘플(A, B)을 뽑습니다.
> 2. 비중 결정: 0에서 1 사이의 값인 $\lambda$를 무작위로 정합니다. (예: $\lambda = 0.6$)
> 3. 데이터 합성
> 	1. 입력(과거 데이터): A 시계열의 값들에 0.6을 곱하고, B 시계열의 값들에 0.4를 곱해서 더합니다.
> 	2. 레이블(미래 예측값): A의 실제 미래값에 0.6을, B의 실제 미래값에 0.4를 곱해서 더합니다.
> 4. 학습: 모델에게 이 '섞인 시계열'을 보여주며 '섞인 미래값'을 예측하게 시킵니다.

> [!NOTE]
> <strong>KernelSynth</strong>
> <strong>시계열을 레고로 보는 방식</strong>
> 복잡해보이는 시계열도 몇 가지 기본패턴으로 쪼갤 수 있음 이 기본패턴을 커널로 정의
> 생성 과정
> 1. 여러개의 커널을 이항연산(+, x) : e.g. RBF + 7일 주기 x 우상향 직선 = 주간 매출 데이터
> 2. 커널의 강도를 조절
> 3. 시계열 생성(GP)

대표적인 '레고 블록(Kernel)'
	RBF (Radial Basis Function) Kernel: 아주 매끄러운 곡선 패턴 (부드러운 변동)
	Periodic Kernel: 일정한 주기로 반복되는 패턴 (계절성)
	Linear Kernel: 일정한 방향으로 올라가거나 내려가는 패턴 (추세)
	White Noise Kernel: 아무 규칙 없는 무작위 떨림(노이즈)

 

---

### 📚 5.6. 하이퍼파라미터 분석 & 5.7. 모델의 한계

### 번역

5.6 하이퍼파라미터 분석

여기서는 다양한 모델 크기와 초기화 방법 비교부터 시작하여, 다운스트림 모델 성능에 대한 여러 설계 선택의 효과를 탐구합니다.
그런 다음 Chronos-T5 (Small)의 성능에 대한 훈련 스텝 수, 합성 데이터 비율, 컨텍스트 길이, 어휘 크기의 효과를 분석합니다.
저희는 관심 있는 파라미터만 변경하고, 다른 모든 것은 주요 결과에 사용된 값으로 고정했습니다.

모델 크기.
저희는 20202020M에서 710710710710M 파라미터 범위의 네 가지 모델 크기를 실험했습니다.
그림 7a에서 볼 수 있듯이, 훈련 손실은 모델 용량이 증가함에 따라 개선되는 것은 놀랍지 않습니다.
그림 7b에서 볼 수 있듯이, 다운스트림 모델 성능에서도 이러한 추세를 관찰할 수 있습니다. 즉, 인-도메인 및 제로샷 벤치마크 모두에서 모델 크기가 커짐에 따라 성능이 향상됩니다.
이러한 추세는 더 큰 모델이 성능을 더욱 향상시킬 수 있음을 시사합니다.
그러나 저희는 추론 시간이 느려져 실제 애플리케이션에 비실용적이게 될 수 있기 때문에 더 큰 모델은 탐색하지 않았습니다.

초기화.
저희는 Chronos 모델을 Tay 외 연구진(2021)이 C4 데이터셋(Raffel 외 연구진, 2020)에서 사전 훈련한 해당 T5 언어 모델로 초기화하는 것이 훈련 동역학 또는 다운스트림 성능에 어떤 영향을 미치는지 조사했습니다.
그림 8은 무작위로 초기화된 모델과 언어 모델 가중치로 초기화된 모델의 훈련 손실 곡선을 보여줍니다.
주목할 점은, 무작위로 초기화된 모델이 언어 모델 가중치로 초기화된 모델에 비해 더 낮은 훈련 손실로 수렴하는 경향이 있다는 것입니다.
더 큰 모델(Base 및 Large)의 경우, 언어 모델 가중치로 초기화된 모델은 초기에 훈련 손실이 더 빠르게 감소하는 것을 보이지만, 궁극적으로는 더 높은 최종 손실로 수렴합니다.

전반적으로 이러한 관찰은 언어 모델 가중치가 시계열 예측 맥락에서 특별히 주목할 만하지 않으며 무작위 초기화에 비해 개선점을 제공하지 않음을 시사합니다.
이러한 결론은 그림 9에서 언어 모델 가중치로 초기화된 모델의 다운스트림 성능과 각 크기의 세 가지 무작위 초기화 모델을 비교한 결과를 통해 더욱 강화됩니다.
모든 모델 크기에 걸쳐, 언어 모델 가중치로 초기화된 모델의 성능은 무작위로 초기화된 모델과 비교했을 때 비슷하거나 약간 낮은 성능을 보입니다.
이러한 결과는 LLM 초기화가 시계열 예측 맥락에서 상대적으로 이점이 적으며, 대신 무작위 초기화가 더 선호되는 선택일 수 있음을 시사합니다.

TSMixup 증강.
섹션 5.2에 설명된 바와 같이, 우리는 Chronos 모델을 원본 시계열 데이터가 아닌 TSMixup 증강 데이터로 학습시켰습니다.
본 실험에서는 TSMixup 증강을 사용하는 것이 다운스트림 성능에 유리한지 조사합니다.
그림 10a는 TSMixup 증강을 사용하거나 사용하지 않고 학습된 Chronos-T5 (Small, 46M) 모델의 성능을 비교합니다.
TSMixup 증강으로 학습된 모델은 증강 없이 학습된 모델과 유사한 인-도메인 성능을 얻습니다.
그러나 TSMixup 증강을 사용할 때 제로샷(zero-shot) 성능이 향상됩니다.
이는 TSMixup이 학습 데이터의 다양성을 향상시켜 보지 못한 데이터셋에 대한 성능을 개선한다는 것을 시사합니다.
그림 10a는 또한 제로샷 성능이 합성 데이터 포함 시 추가적인 향상을 얻는다는 것을 보여줍니다.
다음 실험에서 이를 더 자세히 조사합니다.

합성 데이터 비율.
우리는 KernelSynth가 다운스트림 모델 성능에 미치는 영향을

체계적으로 탐색했습니다.
우리는 TSMixup 증강과 KernelSynth 데이터에서 샘플링된 시계열을 0% (즉, TSMixup 증강으로만 학습)에서 100% 합성 데이터까지 다양한 비율로 사용하여 Chronos-T5 (Small, 46M) 모델을 학습시켰습니다.

그림 10b는 다양한 비율의 합성 데이터로 학습된 모델의 성능을 보여줍니다.
인-도메인 및 제로샷 메트릭 모두 학습 시 합성 데이터 통합으로 개선됩니다.
가장 일관된 개선은 약 10%의 합성 데이터 비율에서 관찰됩니다.

10% 합성 데이터 비율 근처에서 가장 일관된 개선이 관찰됩니다.
합성 데이터 비율을 더 늘리는 것은 성능을 저하시키는 경향이 있습니다.
가우시안 프로세스를 사용하여 생성된 합성 데이터가 모든 실제 시계열을 대표하지 않으므로 이는 놀랍지 않습니다.

합성 데이터로만 훈련된 모델은 실제 데이터를 훈련 코퍼스에 포함한 모델에 비해 성능이 떨어지지만, 절대적 성능 면에서는 합리적으로 잘 수행됩니다.
그림 20 (부록 E)은 이 모델이 ForecastPFN (Dooley et al., 2023)보다 훨씬 더 나은 성능을 보인다는 것을 보여줍니다. ForecastPFN은 (KernelSynth와는 다르게 생성된) 합성 데이터로만 훈련된 또 다른 모델입니다.
놀랍게도, 이 모델은 훈련 중에 실제 데이터를 전혀 보지 못했음에도 불구하고 저희 벤치마크에서 다른 여러 베이스라인보다 우수한 성능을 보입니다.7
이러한 결과는 저희 합성 데이터의 품질을 입증하며, 성능 격차를 더욱 좁히기 위한 향후 연구 방향을 제시합니다.

훈련 단계.
모델 성능에 대한 장기 훈련의 효과를 연구하기 위해 Chronos-T5 (Small, 46M) 모델을 1M 훈련 단계 동안 훈련했습니다.
그림 11a는 다운스트림 모델 성능이 훈련 과정 전반에 걸쳐, 인-도메인 및 제로샷 벤치마크 모두에서 향상됨을 보여줍니다.
이는 더 큰 모델(Base 및 Large)의 성능이 더 오래 훈련함으로써 잠재적으로 향상될 수 있음을 시사합니다.

컨텍스트 길이.
네 가지 다른 컨텍스트 길이를 가진 Chronos-T5 (Small, 46M) 모델을 훈련하여 다운스트림 성능에 대한 컨텍스트 길이의 효과를 연구했습니다.
그림 11b는 컨텍스트 길이가 증가함에 따라 성능이 어떻게 변하는지 보여줍니다.
컨텍스트 길이가 1024까지 증가함에 따라 인-도메인 및 제로샷 메트릭 모두에서 개선이 관찰되었으며, 이는 더 긴 컨텍스트가 어느 정도까지 모델이 더 잘 예측하도록 돕는다는 것을 보여줍니다.

우리는 인-도메인 및 제로샷 메트릭 모두에서 컨텍스트 길이가 1024까지 증가함에 따라 성능 향상을 관찰했으며, 이는 더 긴 컨텍스트가 모델이 특정 정도까지 더 잘 예측하도록 돕는다는 것을 보여줍니다.
그러나 컨텍스트 길이를 더 늘리면 성능이 포화되거나 저하되는 경향이 있는데, 이는 평가 설정의 한계 때문일 수 있습니다. 즉, 충분한 고주파수 데이터셋(>= 15분)을 포함하지 않았기 때문입니다.
따라서 더 긴 컨텍스트 길이의 영향을 확실하게 연구하기 위해서는 추가적인 평가가 필요합니다.
우리는 고주파수 데이터셋이 더 긴 컨텍스트로부터 이점을 얻을 수 있으며, 이는 장기적인 계절 패턴을 올바르게 포착하는 데 필요할 수 있다고 가정합니다.

어휘 크기.
어휘 크기는 모델이 스케일링된 시계열을 처리할 수 있는 정밀도를 결정합니다.
성능에 미치는 영향을 탐색하기 위해 다양한 어휘 크기로 Chronos-T5 (Small, 46M) 모델을 훈련했습니다.
그림 11c는 어휘 크기가 증가함에 따라 포인트 예측 메트릭(MASE)에서 약간의 개선을 보여줍니다.
반면에 WQL은 처음에 개선되다가 더 큰 어휘 크기에서는 성능이 저하됩니다.
우리는 이러한 행동이 선택된 메트릭의 인공적인 결과라고 가정합니다.
개별 시계열의 스케일에 불변하는 MASE는 스케일에 불변하는 훈련 손실과 밀접하게 연관되어 있습니다.
따라서 MASE는 훈련 손실에서 예상되는 바와 같이 정밀도 증가와 함께 개선을 보입니다.
반대로, 스케일에 의존적인 메트릭인 WQL은 훈련 손실과 밀접하게 상관되지 않으며 정밀도가 증가함에 따라 예측 가능성이 떨어집니다.
이러한 메트릭의 속성에 대한 논의는 부록 D를 참조하십시오.
이 실험을 넘어서, 우리는 Chronos와 같은 모델의 맥락에서 어휘 크기를 선택하는 것이 트레이드오프를 제기할 것이라고 가정합니다.
어휘 크기가 너무 작으면 큰 이산화 오류로 인해 예측 정확도가 떨어지지만, 어휘 크기가 크면 빈이 너무 미세해져 각 빈에 들어가는 데이터 포인트 수가 적어 일반화 오류가 발생할 수 있습니다.

5.7 정성적 분석 및 한계 이 섹션에서는 Chronos 모델에서 생성된 예측을 정성적으로 분석하고 토큰화 기법의 몇 가지 한계를 강조합니다.

5.7 정성적 분석 및 한계점 본 섹션에서는 Chronos 모델이 생성한 예측 결과를 정성적으로 분석하고, 저희의 토큰화 기법의 몇 가지 한계점도 강조합니다.
저희는 다양한 시계열 패턴에 대한 통제된 분석을 위해 주로 합성 시계열 데이터에 초점을 맞춥니다.
실제 데이터셋의 예측 결과 예시는 부록 E의 그림 22부터 24까지를 참조하십시오.

I.I.D.
노이즈.
저희는 순전히 가우시안 관측치 N(0, 1) 및 N(100, 10)로 구성된 시계열을 생성하고, 이를 예측하기 위해 Chronos-T5 (Base)를 사용했습니다.
그림 12a는 Chronos가 이러한 시계열에 대해 타당한 예측을 생성하며, 예측된 80% 구간이 점선 파란색 선으로 표시된 실제 80% 구간과 일치함을 보여줍니다.

추세 및 계절성.
저희는 선형 및 지수 추세를 따르는 시계열을 생성했습니다. Chronos-T5 (Base)는 선형 추세는 정확하게 예측하지만, 그림 12b에 표시된 것처럼 지수 추세에는 어려움을 겪습니다.
이는 학습 데이터에서 지수 추세의 표현이 제한적이기 때문일 수 있습니다.
지수 추세가 있는 시계열에 대한 더 나은 예측을 생성하기 위한 잠재적인 해결책은 시계열을 Chronos 모델에 입력하기 전에 로그 스케일링을 수행하는 것입니다.
또한 저희는 컨텍스트가 충분히 길지 않을 때 Chronos 모델이 추세를 과소평가하는 경향이 있음을 관찰했습니다.
이 현상은

짧은 컨텍스트가 제공되었을 때 모델이 패턴은 올바르게 예측하지만 추세를 과소평가하는 그림 13에 묘사되어 있습니다.
하지만 더 긴 컨텍스트를 사용하면 모델은 올바른 패턴과 추세를 파악합니다.
저희 분석에서 Chronos 모델이 시계열에서 계절성 패턴을 특히 잘 인식한다는 것을 관찰했습니다.
저희는 서로 다른 주파수를 가진 사인파를 사용하여 순전히 계절성 시계열을 생성했습니다.
그림 12c에 표시된 것처럼 Chronos-T5 (Base)는 두 시계열 모두를 정확하게 예측합니다.
추세 및 계절성과 같은 근본적인 패턴이 덧셈 또는 곱셈으로 결합될 때, Chronos는 이를 정확하게 예측합니다.
이는 사인파와 선형 함수의 덧셈 및 곱셈을 통해 생성된 시계열에 대한 그림 12d에서 입증됩니다.

자기회귀 프로세스.
차수 p의 자기회귀(AR) 프로세스는 다음과 같이 정의됩니다.

$$
X_t = \sum_{i=1}^{p} \phi_i X_{t-i} + \varepsilon_t
$$

여기서 $\varepsilon_t \sim N(0, 1)$이고 $\phi_1, \dots, \phi_p$는 모델의 파라미터입니다.
저희는 1부터 4까지의 다양한 차수를 가진 정상 자기회귀 프로세스에서 시계열을 생성했으며, Chronos-T5 (Base)가 생성한 예측을 다음 세 가지 모델의 예측과 비교했습니다: (a) 시계열을 생성하는 데 사용된 실제 AR 모델; (b) 시계열에 맞춰진 올바른 차수(p)를 가진 AR 모델; (c) 시계열에 맞춰진 AutoARIMA 모델.
그림 14는 AR(1) 및 AR(4) 프로세스에 대한 결과를 보여주고, 그림 21(부록 E)은 AR(2) 및 AR(3)에 대한 결과를 보여줍니다.
저희는 Chronos-T5 (Base)가 네 가지 AR 프로세스 모두에서 타당한 예측을 생성함을 관찰했습니다.
더 간단한 AR(1) 및 AR(2) 프로세스는 올바르게 지정된 AR 모델과 AutoARIMA 모델이 더 쉽게 맞출 수 있어, Chronos-T5 (Base)보다 더 나은 MSE를 얻습니다.
하지만 AR(3) 및 AR(4) 프로세스의 복잡성이 증가함에 따라, Chronos-T5 (Base)는 AutoARIMA 모델(실제 모델과 동일한 계열에 속함)을 능가할 뿐만 아니라 올바른 차수로 맞춰진 AR 모델과도 동등한 성능을 보입니다.
이러한 결과는 Chronos 모델이 시계열 데이터에 존재하는 근본적인 패턴을 인식할 수 있음을 강조합니다.

유연한 예측 분포.
범주형 분포를 사용하여 예측을 인코딩함으로써 Chronos는 다양한 형태의 예측 분포를 생성하는 데 유연성을 제공합니다.
이는 그림 15에 표시되어 있으며, 세 가지 데이터셋에 걸쳐 예측 호라이즌의 처음 다섯 타임스텝에 대해 Chronos 모델에서 샘플링된 토큰 ID의 커널 밀도 추정(KDE) 플롯을 보여줍니다.
교차 엔트로피가 거리 인식 기능이 없음에도 불구하고, Chronos는 연속적인 토큰 집합에 걸쳐, 그리고 다중 모달을 포함한 다양한 형태의 예측 분포를 출력합니다.

교차 엔트로피가 거리 인식 기능을 갖지 않음에도 불구하고, Chronos는 다중 모드를 포함하여 다양한 형태의 연속적인 토큰 집합에 대한 예측 분포를 출력합니다.
Chronos는 데이터로부터 직접 공간의 위상을 학습하지만, 학습 중에 모델에 명시적인 위상 정보를 제공하면 프로세스가 가속화되고 데이터 포인트가 적은 토큰에 대해 모델이 견고해질 수 있다고 가정합니다.
교차 엔트로피 손실에 위상 정보를 주입하는 한 가지 잠재적인 방법은 일종의 레이블 스무딩(label smoothing)을 통하는 것인데, 이는 올바른 토큰의 이웃에 있는 토큰(즉, 빈)에 0이 아닌 확률 질량을 할당하는 것입니다.
Farebrother 등(2024)은 강화 학습 맥락에서 이러한 거리 인식 회귀-분류(regression-via-classification) 목표를 사용하여 유망한 결과를 얻었습니다.
시계열 예측 맥락에서 회귀-분류 패러다임에 대한 심층적인 이론적 및 경험적 분석은 흥미로운 향후 연구를 구성할 것입니다.

오버플로우 및 정밀도 손실.
Chronos의 한 가지 한계는 제안된 토큰화 접근 방식(섹션 3.1 참조)에서 비롯됩니다.
구체적으로, 우리가 선택한 토큰은 범위 [−15, +15] 내의 빈 중심을 나타내며, 이는 궁극적으로 원래 시계열 값을 범위 [−15s, 15s]로 나타냅니다. 여기서 s는 시계열의 스케일(평균 절대값)입니다.
만약 s가 해당 시리즈의 값 범위에 비해 매우 작다면, 일부 관측치는 표현 가능한 범위를 벗어나게 됩니다.

만약 s가 시계열 값의 범위에 비해 매우 작다면, 일부 관측치는 표현 가능한 범위를 벗어나게 됩니다.
이러한 동작의 한 예는 희소 시계열에서 나타나며, 그림 16a에 표시되어 있습니다.
반면에, 분산에 비해 s 값이 매우 크면 정밀도 손실이 발생합니다. 원래 공간에서 토큰은 서로 $30s/(B - 1)$ 간격으로 떨어져 있으며, 여기서 B는 빈(bin)의 개수입니다 (실험에서는 B = 4094를 사용했습니다). 이보다 가까운 값들은 동일한 토큰으로 매핑될 수 있으며, 이는 명백한 정밀도 손실을 초래합니다.
이러한 동작의 한 예는 그림 16b에 제시되어 있습니다.
이 문제에 대한 추론 시점 휴리스틱 해결책은 대규모이고 분산이 작은 시계열에 대해 표준화와 같은 대안적인 정규화 방식을 사용하여 시계열을 사전 처리하는 것입니다.
휴리스틱 없이 이러한 엣지 케이스를 극복하기 위해 토큰화(tokenization)를 개선하는 것은 향후 연구 과제이지만, 섹션 5.5의 결과는 Chronos 모델이 이러한 한계에도 불구하고 실제 데이터에서 잘 작동함을 시사합니다.


---
### 내용



---
### 포인트


---
---
